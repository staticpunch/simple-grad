{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad1a6fcd-1fda-4ef6-92cd-136a0a608394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77a7b90a-deb7-4ed2-840b-37cd375a8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e50e752b-1978-4172-8782-f30d821c2ca2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msimplegrad\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msimplegrad\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmy_tests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m test_ops\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import simplegrad as sg\n",
    "from simplegrad import Tensor\n",
    "from simplegrad import ops\n",
    "from ..my_tests import test_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5fa2d300-30e7-45cc-8595-8afdf8a12443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 1, 4]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ishape, oshape = (3, 1, 4), (4, 3, 5, 4)\n",
    "aligned = [1] * (len(oshape) - len(ishape)) + list(ishape)\n",
    "axes = tuple([i for i, axis in enumerate(aligned) if axis == 1])\n",
    "aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2bd6381a-aa1e-44ee-846a-34a27e2b0765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1] * (len(oshape) - len(ishape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6dc82f91-bebe-4fa7-a332-ff953a1194b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(tensor):\n",
    "    out = Tensor(np.log(tensor.data), requires_grad=tensor.requires_grad)\n",
    "    def _backward():\n",
    "        tensor.grad += out.grad / tensor.data\n",
    "    out._backward = _backward\n",
    "    out._prev = {tensor, }\n",
    "    return out\n",
    "\n",
    "def exp(tensor):\n",
    "    out = Tensor(np.exp(tensor.data), requires_grad=tensor.requires_grad)\n",
    "    \n",
    "    def _backward():\n",
    "        if tensor.requires_grad:\n",
    "            tensor.grad += out.data * out.grad \n",
    "    \n",
    "    out._backward = _backward\n",
    "    out._prev = {tensor, }\n",
    "    return out\n",
    "\n",
    "def summation(tensor, axis=None, keepdims=False):\n",
    "    \"\"\"\n",
    "    local_grad = d.sum(x) / d.xi = 1.\n",
    "    therefore derivative of x is basically just out.grad\n",
    "    broadcasted to the shape of the input tensor.\n",
    "    \"\"\"\n",
    "    out = Tensor(np.sum(tensor.data, axis=axis, keepdims=keepdims), requires_grad=tensor.requires_grad)\n",
    "    def _backward():\n",
    "        if tensor.requires_grad:\n",
    "            input_shape, axes = tensor.data.shape, axis\n",
    "            \n",
    "            if not keepdims:\n",
    "                if axis is None: # if self.axes is None, take sum over all axes.\n",
    "                    axes = tuple(i for i in range(len(input_shape)))\n",
    "                elif isinstance(axis, int): \n",
    "                    axes = (axis,)\n",
    "\n",
    "                shape_range = range(len(input_shape))\n",
    "                mask = np.array([0 if i in axes else 1 for i in shape_range])\n",
    "                new_shape = np.array(input_shape) * mask + (1 - mask)\n",
    "                grad = np.reshape(out.grad, new_shape)\n",
    "                grad = np.broadcast_to(grad, input_shape)\n",
    "            else:\n",
    "                grad = np.broadcast_to(out.grad, input_shape)\n",
    "                \n",
    "            tensor.grad += grad\n",
    "        \n",
    "    out._backward = _backward\n",
    "    out._prev = {tensor,}\n",
    "    return out\n",
    "    \n",
    "def broadcast_to(tensor, shape):\n",
    "    \"\"\"this is interestingly the reverse of summation.\"\"\"\n",
    "    if tensor.shape == shape: # Optimization: no-op if shapes match\n",
    "        return tensor\n",
    "        \n",
    "    out_data = np.broadcast_to(tensor.data, shape)\n",
    "    out = Tensor(out_data, requires_grad=tensor.requires_grad)\n",
    "    \n",
    "    input_shape = tensor.shape # Capture input shape for backward pass\n",
    "    def _backward():\n",
    "        if tensor.requires_grad:\n",
    "            ishape, oshape = tensor.data.shape, out.grad.shape\n",
    "            ## in = (3, 1, 4), out = (3, 5, 4) -> aligned = (3, 1, 4)\n",
    "            ## i think numpy only implicitly broadcast to prefix dims :/\n",
    "            aligned = [1] * (len(oshape) - len(ishape)) + list(ishape)\n",
    "            broadcast_axes = tuple([i for i, axis in enumerate(aligned) if axis == 1])\n",
    "            grad = np.sum(out.grad, axis=broadcast_axes, keepdims=True)\n",
    "            grad = np.reshape(grad, ishape)\n",
    "\n",
    "            tensor.grad += grad\n",
    "        \n",
    "    out._backward = _backward\n",
    "    out._prev = {tensor,}\n",
    "    return out\n",
    "    \n",
    "def logsumexp(tensor, axis=None, keepdims=False):\n",
    "    \"\"\"\n",
    "    mathematical operations, applied to 1D vector: \n",
    "    forward: log(e^z1 + e^z2 + ... + e^zn) = sum(e^zi)\n",
    "    backward: local_grad[i] = e^zi / sum(e^zi)\n",
    "    ------\n",
    "    for numerical stability:\n",
    "    forward: log(sum(e^zi))  = log(sum(e^(zi - zmax)) * e^zmax)\n",
    "                             = log(sum(e^(zi - zmax))) + log(e^zmax)\n",
    "    backward: e^zi/sum(e^zi) = e^(zi - zmax) / sum(e^(zi - zmax))\n",
    "    \"\"\"\n",
    "    max_z = np.max(tensor.data, axis=axis, keepdims=True)\n",
    "    stable_z = tensor.data - max_z\n",
    "    exp_stable_z = np.exp(stable_z)\n",
    "    stable_sum = np.sum(exp_stable_z, axis=axis, keepdims=keepdims)\n",
    "    max_term = max_z if keepdims else np.squeeze(max_z, axis=axis)\n",
    "    data = np.log(stable_sum) + max_term\n",
    "    out = Tensor(data, requires_grad=tensor.requires_grad)\n",
    "    \n",
    "    def _backward():\n",
    "        if tensor.requires_grad:\n",
    "            if axis is None:\n",
    "                # For None axis, basically all dims.\n",
    "                if not keepdims:\n",
    "                    grad_shaped = out.grad * np.ones_like(tensor.data)\n",
    "                    softmax_terms = exp_stable_z / np.sum(exp_stable_z)\n",
    "                    tensor.grad += grad_shaped * softmax_terms\n",
    "                else:\n",
    "                    # keepdims=True with axis=None\n",
    "                    softmax_terms = exp_stable_z / stable_sum\n",
    "                    tensor.grad += out.grad * softmax_terms\n",
    "            else:\n",
    "                # For specific axis reduction\n",
    "                grad_shaped = out.grad\n",
    "                if not keepdims:\n",
    "                    grad_shaped = np.expand_dims(grad_shaped, axis=axis)\n",
    "    \n",
    "                denom = stable_sum if keepdims \\\n",
    "                        else np.expand_dims(stable_sum, axis=axis)\n",
    "                softmax_terms = exp_stable_z / denom\n",
    "                tensor.grad += grad_shaped * softmax_terms\n",
    "    \n",
    "    out._backward = _backward\n",
    "    out._prev = {tensor, }\n",
    "    return out\n",
    "\n",
    "\"\"\"\n",
    "    1. note: id(i, j) = 1{i == j}\n",
    "    mathematical operations, applied to 1D vector:\n",
    "    \n",
    "    forward: softmax(z)[i] = e^zi / sum(e^z)\n",
    "    backward: since softmax is a vector-to-vector function,\n",
    "              the local_grad we need to compute is a Jacobian:\n",
    "        local_grad[i,j] = softmax(z)[i] * (id(i,j) - softmax(z)[j])\n",
    "    \n",
    "    \n",
    "    2. for numerical stability. \n",
    "    forward: softmax(z)[i] = e^zi / sum(e^z)\n",
    "             = e^(zi) / e^(logsumexp(z))\n",
    "             = e^(zi - logsumexp(z))\n",
    "             \n",
    "    backward (vectorized): \n",
    "        let ID.shape = local_grad.shape = (N, N). \n",
    "            ID[i, j] = id(i, j).\n",
    "            local_grad[i, j] = d.s[i] / d.z[j]\n",
    "            \n",
    "        local_grad[i, j] = out[i] * ID[i, j] - out[i] * out[j]\n",
    "        local_grad[i, :] = out[i] * ID[i, :] - out[i] * out[:]\n",
    "        local_grad[:, :] = out[:] * ID[:, :] - out[:, None] * out[None, :]\n",
    "                         = diag(out) - outer(out, out)\n",
    "\n",
    "        shit, however, this is not really efficient.\n",
    "\"\"\"\n",
    "def softmax(tensor, axis: int = None):\n",
    "    \"\"\"\n",
    "    to reduce headache, actually I should implement an exp ops,\n",
    "    then let the chain rule do its job automatically.\n",
    "    \"\"\"\n",
    "    lse = logsumexp(tensor, axis=axis, keepdims=True)\n",
    "    # print(tensor.shape, lse.shape)\n",
    "    lse_broadcast = broadcast_to(lse, tensor.data.shape)\n",
    "    log_softmax = tensor - lse_broadcast\n",
    "    out = exp(log_softmax)\n",
    "          \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1f149e29-a0bb-4c19-a273-ed82f658b60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LogSumExp operation...\n",
      "LogSumExp forward test 1 passed!\n",
      "LogSumExp backward test 1 passed!\n",
      "LogSumExp forward test 2 passed!\n",
      "LogSumExp backward test 2 passed!\n",
      "LogSumExp forward test 3 passed!\n",
      "LogSumExp backward test 3 passed!\n",
      "LogSumExp forward test 4 passed!\n",
      "LogSumExp backward test 4 passed!\n",
      "LogSumExp forward test 5 passed!\n",
      "LogSumExp backward test 5 passed!\n",
      "LogSumExp forward test 6 passed!\n",
      "LogSumExp backward test 6 passed!\n",
      "LogSumExp forward test 7 passed!\n",
      "LogSumExp backward test 7 passed!\n",
      "LogSumExp forward test 8 passed!\n",
      "LogSumExp backward test 8 passed!\n",
      "LogSumExp forward test 9 passed!\n",
      "LogSumExp backward test 9 passed!\n",
      "LogSumExp forward test 10 passed!\n",
      "LogSumExp backward test 10 passed!\n",
      "\n",
      "Testing LogSumExp specific cases...\n",
      "LogSumExp specific case 1 (large identical values) passed!\n",
      "LogSumExp specific case 2 (extreme value differences) passed!\n",
      "LogSumExp specific case 3 (softmax relation) passed!\n",
      "\n",
      "All LogSumExp tests completed successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing LogSumExp operation...\")\n",
    "test_logsumexp()\n",
    "\n",
    "print(\"\\nTesting LogSumExp specific cases...\")\n",
    "test_logsumexp_specific_cases()\n",
    "\n",
    "print(\"\\nAll LogSumExp tests completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9678ed6e-5371-4b5e-8810-e1014fb3ddd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "broadcast_to forward test 1 passed!\n",
      "broadcast_to backward test 1 passed!\n",
      "broadcast_to forward test 2 passed!\n",
      "broadcast_to backward test 2 passed!\n",
      "broadcast_to forward test 3 passed!\n",
      "broadcast_to backward test 3 passed!\n",
      "broadcast_to forward test 4 passed!\n",
      "broadcast_to backward test 4 passed!\n",
      "broadcast_to forward test 5 passed!\n",
      "broadcast_to backward test 5 passed!\n",
      "broadcast_to forward test 6 passed!\n",
      "broadcast_to backward test 6 passed!\n",
      "broadcast_to forward test 7 passed!\n",
      "broadcast_to backward test 7 passed!\n",
      "broadcast_to forward test 8 passed!\n",
      "broadcast_to backward test 8 passed!\n",
      "broadcast_to forward test 9 passed!\n",
      "broadcast_to backward test 9 passed!\n",
      "broadcast_to forward test 10 passed!\n",
      "broadcast_to backward test 10 passed!\n"
     ]
    }
   ],
   "source": [
    "test_broadcast_to()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6e6adc0d-a893-447b-bcc5-b7e02a6a6b87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Softmax operation...\n",
      "Softmax forward test 1 passed!\n",
      "Softmax backward test 1 passed!\n",
      "Softmax forward test 2 passed!\n",
      "Softmax backward test 2 passed!\n",
      "Softmax forward test 3 passed!\n",
      "Softmax backward test 3 passed!\n",
      "Softmax forward test 4 passed!\n",
      "Softmax backward test 4 passed!\n",
      "Softmax forward test 5 passed!\n",
      "Softmax backward test 5 passed!\n",
      "Softmax forward test 6 passed!\n",
      "Softmax backward test 6 passed!\n",
      "Softmax forward test 7 passed!\n",
      "Softmax backward test 7 passed!\n",
      "Softmax forward test 8 passed!\n",
      "Softmax backward test 8 passed!\n",
      "Softmax forward test 9 passed!\n",
      "Softmax backward test 9 passed!\n",
      "Softmax forward test 10 passed!\n",
      "Softmax backward test 10 passed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Softmax operation...\")\n",
    "test_softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f13b87f7-09e1-4273-b2a6-866e286a9b89",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from simplegrad.tensor import Tensor\n",
    "\n",
    "\n",
    "def test_logsumexp():\n",
    "    # Define diverse test cases\n",
    "    test_cases = [\n",
    "        # Small values\n",
    "        {\"data\": np.random.rand(3, 4) * 0.001, \"axis\": None, \"keepdims\": False},\n",
    "        # Large values\n",
    "        {\"data\": np.random.rand(3, 4) * 100, \"axis\": None, \"keepdims\": False},\n",
    "        # Negative values\n",
    "        {\"data\": np.random.rand(3, 4) * -10, \"axis\": None, \"keepdims\": False},\n",
    "        # Mixed values\n",
    "        {\"data\": np.random.rand(3, 4) * 2 - 1, \"axis\": None, \"keepdims\": False},\n",
    "        # Single dimension reduction with keepdims=True\n",
    "        {\"data\": np.random.rand(3, 4) * 2 - 1, \"axis\": 0, \"keepdims\": True},\n",
    "        # Single dimension reduction with keepdims=False\n",
    "        {\"data\": np.random.rand(3, 4) * 2 - 1, \"axis\": 1, \"keepdims\": False},\n",
    "        # Multiple dimensions\n",
    "        {\"data\": np.random.rand(2, 3, 4) * 2 - 1, \"axis\": None, \"keepdims\": False},\n",
    "        # Multiple dimensions with specific axis\n",
    "        {\"data\": np.random.rand(2, 3, 4) * 2 - 1, \"axis\": 1, \"keepdims\": False},\n",
    "        # Multiple dimensions with tuple axis\n",
    "        {\"data\": np.random.rand(2, 3, 4) * 2 - 1, \"axis\": (0, 2), \"keepdims\": False},\n",
    "        # Multiple dimensions with tuple axis and keepdims=True\n",
    "        {\"data\": np.random.rand(2, 3, 4) * 2 - 1, \"axis\": (0, 2), \"keepdims\": True}\n",
    "    ]\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        data = test_case[\"data\"]\n",
    "        axis = test_case[\"axis\"]\n",
    "        keepdims = test_case[\"keepdims\"]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "        x = Tensor.from_torch(pt_x)\n",
    "        \n",
    "        # PyTorch version\n",
    "        if axis is None:\n",
    "            # PyTorch's logsumexp requires a specific dim\n",
    "            expected = torch.logsumexp(pt_x, dim=tuple(range(pt_x.dim())), keepdim=keepdims)\n",
    "        elif isinstance(axis, int):\n",
    "            expected = torch.logsumexp(pt_x, dim=axis, keepdim=keepdims)\n",
    "        else:\n",
    "            # For multiple axes, we need to handle them one by one in PyTorch\n",
    "            temp = pt_x\n",
    "            # Process axes in reverse order to maintain correct dimensions\n",
    "            for ax in sorted(axis, reverse=True):\n",
    "                temp = torch.logsumexp(temp, dim=ax, keepdim=keepdims)\n",
    "            expected = temp\n",
    "        \n",
    "        # Our implementation\n",
    "        result = logsumexp(x, axis=axis, keepdims=keepdims)\n",
    "        \n",
    "        # Check forward pass\n",
    "        np.testing.assert_allclose(\n",
    "            result.data, \n",
    "            expected.detach().numpy(), \n",
    "            rtol=1e-5, atol=1e-5,\n",
    "            err_msg=f\"Forward pass failed for test case {i+1}: data shape {data.shape}, axis {axis}, keepdims {keepdims}\"\n",
    "        )\n",
    "        print(f\"LogSumExp forward test {i+1} passed!\")\n",
    "        \n",
    "        # Compute gradients\n",
    "        grad_output = torch.ones_like(expected)\n",
    "        expected.backward(grad_output)\n",
    "        result.backward()\n",
    "        \n",
    "        # Check backward pass\n",
    "        np.testing.assert_allclose(\n",
    "            x.grad, \n",
    "            pt_x.grad.detach().numpy(), \n",
    "            rtol=1e-5, atol=1e-5,\n",
    "            err_msg=f\"Backward pass failed for test case {i+1}: data shape {data.shape}, axis {axis}, keepdims {keepdims}\"\n",
    "        )\n",
    "        print(f\"LogSumExp backward test {i+1} passed!\")\n",
    "\n",
    "\n",
    "def test_logsumexp_specific_cases():\n",
    "    \"\"\"Test specific edge cases for logsumexp\"\"\"\n",
    "    \n",
    "    # Case 1: All elements are the same (tests numerical stability)\n",
    "    data = np.ones((3, 3)) * 1000  # Large identical values\n",
    "    pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "    x = Tensor.from_torch(pt_x)\n",
    "    \n",
    "    expected = torch.logsumexp(pt_x, dim=1, keepdim=False)\n",
    "    result = logsumexp(x, axis=1, keepdims=False)\n",
    "    \n",
    "    np.testing.assert_allclose(result.data, expected.detach().numpy(), rtol=1e-5, atol=1e-5)\n",
    "    print(\"LogSumExp specific case 1 (large identical values) passed!\")\n",
    "    \n",
    "    # Case 2: Extreme differences between values (tests numerical stability)\n",
    "    data = np.array([[1e-10, 1e10], [1e-10, 1e-10]])\n",
    "    pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "    x = Tensor.from_torch(pt_x)\n",
    "    \n",
    "    expected = torch.logsumexp(pt_x, dim=1, keepdim=False)\n",
    "    result = logsumexp(x, axis=1, keepdims=False)\n",
    "    \n",
    "    np.testing.assert_allclose(result.data, expected.detach().numpy(), rtol=1e-5, atol=1e-5)\n",
    "    print(\"LogSumExp specific case 2 (extreme value differences) passed!\")\n",
    "    \n",
    "    # Case 3: Test with softmax relation (logsumexp is used in softmax implementation)\n",
    "    data = np.random.rand(5, 10) * 2 - 1\n",
    "    pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "    x = Tensor.from_torch(pt_x)\n",
    "    \n",
    "    # Standard softmax calculation using logsumexp\n",
    "    pt_logsumexp = torch.logsumexp(pt_x, dim=1, keepdim=True)\n",
    "    pt_softmax = torch.exp(pt_x - pt_logsumexp)\n",
    "    \n",
    "    our_logsumexp = logsumexp(x, axis=1, keepdims=True)\n",
    "    our_softmax = np.exp(x.data - our_logsumexp.data)\n",
    "    \n",
    "    np.testing.assert_allclose(our_softmax, pt_softmax.detach().numpy(), rtol=1e-5, atol=1e-5)\n",
    "    print(\"LogSumExp specific case 3 (softmax relation) passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2302051b-78e7-4201-9bb8-74a4a914583b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def test_broadcast_to():\n",
    "    # Define diverse test cases\n",
    "    test_cases = [\n",
    "        # Broadcast single value to vector\n",
    "        {\"data\": np.array([1.0]), \"shape\": (5,)},\n",
    "        # Broadcast vector to matrix (adding explicit dimension for PyTorch compatibility)\n",
    "        {\"data\": np.random.rand(1, 3), \"shape\": (3, 3)},\n",
    "        # Broadcast row vector to matrix\n",
    "        {\"data\": np.random.rand(1, 4), \"shape\": (3, 4)},\n",
    "        # Broadcast column vector to matrix\n",
    "        {\"data\": np.random.rand(3, 1), \"shape\": (3, 4)},\n",
    "        # Broadcast scalar to matrix\n",
    "        {\"data\": np.array([1.0]), \"shape\": (3, 4)},\n",
    "        # Broadcast to higher dimensions\n",
    "        {\"data\": np.random.rand(2, 1, 3), \"shape\": (2, 5, 3)},\n",
    "        # No broadcasting (same shape)\n",
    "        {\"data\": np.random.rand(2, 3), \"shape\": (2, 3)},\n",
    "        # Broadcast in middle dimension\n",
    "        {\"data\": np.random.rand(2, 1, 4), \"shape\": (2, 3, 4)},\n",
    "        # Multiple dimensions broadcasted\n",
    "        {\"data\": np.random.rand(1, 1, 3), \"shape\": (2, 4, 3)},\n",
    "        # Broadcasting with various data values\n",
    "        {\"data\": np.random.rand(1, 3) * 10 - 5, \"shape\": (4, 3)},\n",
    "    ]\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        data = test_case[\"data\"]\n",
    "        shape = test_case[\"shape\"]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "        x = Tensor(data, requires_grad=True)\n",
    "        \n",
    "        # PyTorch version\n",
    "        expected = pt_x.expand(shape)\n",
    "        \n",
    "        # Our implementation\n",
    "        result = broadcast_to(x, shape)\n",
    "        \n",
    "        # Check forward pass\n",
    "        np.testing.assert_allclose(\n",
    "            result.data, \n",
    "            expected.detach().numpy(), \n",
    "            rtol=1e-5, atol=1e-6,\n",
    "            err_msg=f\"broadcast_to Forward pass failed for test case {i+1}: data shape {data.shape}, target shape {shape}\"\n",
    "        )\n",
    "        print(f\"broadcast_to forward test {i+1} passed!\")\n",
    "        \n",
    "        # Generate random gradient for backward pass\n",
    "        grad_output_np = np.random.rand(*shape).astype(np.float32)\n",
    "        grad_output_torch = torch.tensor(grad_output_np)\n",
    "        \n",
    "        # Compute gradients\n",
    "        expected.backward(grad_output_torch)\n",
    "        \n",
    "        # With our updated backward method, we can pass the gradient directly:\n",
    "        result.backward(grad_output_np)\n",
    "        \n",
    "        # Check backward pass\n",
    "        np.testing.assert_allclose(\n",
    "            x.grad, \n",
    "            pt_x.grad.detach().numpy(), \n",
    "            rtol=1e-5, atol=1e-5,\n",
    "            err_msg=f\"broadcast_to Backward pass failed for test case {i+1}: data shape {data.shape}, target shape {shape}\"\n",
    "        )\n",
    "        print(f\"broadcast_to backward test {i+1} passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "41f6bd74-6050-4a34-9459-2b972e207c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.08273586)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float32(np.random.rand())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "10584828-8ce0-48a6-9023-c3bda023b88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.e+15, 1.e-15, 1.e-15, 1.e+15])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ndarray(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9dac3451-e2bf-4d65-b4cc-1d55eb24c990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.4400)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(5.44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "3fffe8d5-51de-41e8-897b-ca9003f13e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = {\"data\": np.random.rand(5, 5), \"axis\": 1, \"keepdims\": True, \"name\": \"Basic 2D, axis 1 with keepdims\"}\n",
    "data = test_case[\"data\"]\n",
    "axis = test_case[\"axis\"]\n",
    "keepdims = test_case[\"keepdims\"]\n",
    "name = test_case[\"name\"]\n",
    "\n",
    "pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "x = Tensor(data, requires_grad=True)\n",
    "\n",
    "result = summation(x, axis=axis, keepdims=keepdims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a4aa576e-6318-4c31-aa56-6e1d41025627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Tensor([[0.29151535 0.4114775  0.15306664 0.12506993 0.24554916]\n",
       "  [0.9562889  0.5814308  0.21557261 0.2233879  0.9167526 ]\n",
       "  [0.75078666 0.49505213 0.8327872  0.6036786  0.17937946]\n",
       "  [0.42289641 0.71969664 0.03585196 0.6252484  0.32845837]\n",
       "  [0.23447901 0.07065094 0.81188375 0.7233557  0.7745351 ]], requires_grad=True),\n",
       " Tensor([[1.2266786]\n",
       "  [2.8934329]\n",
       "  [2.8616838]\n",
       "  [2.1321516]\n",
       "  [2.6149046]], requires_grad=True))"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "321b6802-098b-4898-a848-7dc476397843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2267],\n",
       "        [2.8934],\n",
       "        [2.8617],\n",
       "        [2.1322],\n",
       "        [2.6149]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_x.sum(dim=axis, keepdim=keepdims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6487915f-6905-4d74-bdab-b604e586f2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_summation():\n",
    "    \"\"\"Test the summation operation with challenging cases\"\"\"\n",
    "    print(\"\\n=== TESTING SUMMATION ===\")\n",
    "    \n",
    "    # Define challenging test cases\n",
    "    test_cases = [\n",
    "        # Basic cases\n",
    "        {\"data\": np.random.rand(5, 5), \"axis\": None, \"keepdims\": False, \"name\": \"Basic 2D, all axes\"},\n",
    "        {\"data\": np.random.rand(5, 5), \"axis\": 0, \"keepdims\": False, \"name\": \"Basic 2D, axis 0\"},\n",
    "        {\"data\": np.random.rand(5, 5), \"axis\": 1, \"keepdims\": True, \"name\": \"Basic 2D, axis 1 with keepdims\"},\n",
    "        \n",
    "        # Extreme values\n",
    "        {\"data\": np.random.rand(10, 10) * 1e10, \"axis\": None, \"keepdims\": False, \"name\": \"Large values (1e10)\"},\n",
    "        {\"data\": np.random.rand(10, 10) * 1e-10, \"axis\": 0, \"keepdims\": True, \"name\": \"Small values (1e-10)\"},\n",
    "        {\"data\": np.array([[1e15, 1e-15], [1e-15, 1e15]]), \"axis\": 1, \"keepdims\": False, \"name\": \"Mixed extreme values\"},\n",
    "        \n",
    "        # Large dimensions\n",
    "        {\"data\": np.random.rand(1000, 5), \"axis\": 0, \"keepdims\": False, \"name\": \"Large first dimension (1000x5)\"},\n",
    "        {\"data\": np.random.rand(5, 1000), \"axis\": 1, \"keepdims\": True, \"name\": \"Large second dimension (5x1000)\"},\n",
    "        \n",
    "        # Higher dimensions\n",
    "        {\"data\": np.random.rand(10, 10, 10), \"axis\": (0, 2), \"keepdims\": False, \"name\": \"3D with multiple axes\"},\n",
    "        {\"data\": np.random.rand(5, 5, 5, 5), \"axis\": (1, 2), \"keepdims\": True, \"name\": \"4D with multiple axes and keepdims\"},\n",
    "        \n",
    "        # Special patterns\n",
    "        {\"data\": np.ones((20, 20)), \"axis\": None, \"keepdims\": False, \"name\": \"All ones\"},\n",
    "        {\"data\": np.zeros((20, 20)), \"axis\": 0, \"keepdims\": True, \"name\": \"All zeros\"},\n",
    "        {\"data\": np.eye(20), \"axis\": 1, \"keepdims\": False, \"name\": \"Identity matrix\"},\n",
    "        \n",
    "        # Edge cases\n",
    "        {\"data\": np.array([1.0]), \"axis\": None, \"keepdims\": False, \"name\": \"Single value\"},\n",
    "        {\"data\": np.random.rand(1, 1, 1, 1), \"axis\": (1, 2), \"keepdims\": True, \"name\": \"Multiple singleton dimensions\"},\n",
    "    ]\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        data = test_case[\"data\"]\n",
    "        axis = test_case[\"axis\"]\n",
    "        keepdims = test_case[\"keepdims\"]\n",
    "        name = test_case[\"name\"]\n",
    "        \n",
    "        # Create tensors\n",
    "        pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "        x = Tensor(data, requires_grad=True)\n",
    "        \n",
    "        # PyTorch sum\n",
    "        if isinstance(axis, tuple):\n",
    "            # For multiple axes in PyTorch, we need to do them one by one\n",
    "            expected = pt_x\n",
    "            for ax in sorted(axis, reverse=True):  # Start from the highest axis\n",
    "                expected = expected.sum(dim=ax, keepdim=keepdims)\n",
    "        else:\n",
    "            expected = pt_x.sum(dim=axis, keepdim=keepdims)\n",
    "        \n",
    "        # Our summation\n",
    "        result = summation(x, axis=axis, keepdims=keepdims)\n",
    "        \n",
    "        # Check forward pass\n",
    "        try:\n",
    "            np.testing.assert_allclose(\n",
    "                result.data,\n",
    "                expected.detach().numpy(),\n",
    "                rtol=1e-5, atol=1e-5,\n",
    "                err_msg=f\"Summation forward pass failed\"\n",
    "            )\n",
    "            # print(f\"  ✓ Forward pass successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Forward pass failed: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Generate random gradient for backward pass\n",
    "        grad_output_np = np.random.rand(*result.data.shape)\n",
    "        if isinstance(grad_output_np, float):\n",
    "            grad_output_np = np.float32(grad_output_np)\n",
    "        else:\n",
    "            grad_output_np = grad_output_np.astype(np.float32)\n",
    "        grad_output_torch = torch.tensor(grad_output_np)\n",
    "        \n",
    "        # Compute gradients\n",
    "        expected.backward(grad_output_torch)\n",
    "        result.backward(grad_output_np)\n",
    "        \n",
    "        # Check backward pass\n",
    "        try:\n",
    "            np.testing.assert_allclose(\n",
    "                x.grad,\n",
    "                pt_x.grad.detach().numpy(),\n",
    "                rtol=1e-4, atol=1e-5,\n",
    "                err_msg=f\"broadcast_to backward pass failed\"\n",
    "            )\n",
    "            # print(f\"  ✓ Backward pass successful\")\n",
    "            result_msg = \"Successful.\"\n",
    "        except Exception as e:\n",
    "            result_msg = \"Failed.\"\n",
    "            print(f\"  ✗ Backward pass failed: {e}\")\n",
    "        \n",
    "        # Reset gradients\n",
    "        pt_x.grad = None\n",
    "        x.grad = np.zeros_like(x.data)\n",
    "        \n",
    "        print(\n",
    "            f\"Test case {i+1}: {name}.\"\n",
    "            # f\" Shape: {data.shape}, Axis: {axis}, Keepdims: {keepdims}.\"\n",
    "            f\" {result_msg}\"\n",
    "        )\n",
    "\n",
    "def test_broadcast_to():\n",
    "    \"\"\"Test the broadcast_to operation with challenging cases\"\"\"\n",
    "    print(\"\\n=== TESTING BROADCAST_TO ===\")\n",
    "    \n",
    "    # Define challenging test cases\n",
    "    test_cases = [\n",
    "        # Basic broadcasting\n",
    "        {\"data\": np.random.rand(1), \"shape\": (10,), \"name\": \"Scalar to vector\"},\n",
    "        {\"data\": np.random.rand(1, 5), \"shape\": (10, 5), \"name\": \"Row to matrix\"},\n",
    "        {\"data\": np.random.rand(5, 1), \"shape\": (5, 10), \"name\": \"Column to matrix\"},\n",
    "        \n",
    "        # Extreme values\n",
    "        {\"data\": np.random.rand(1, 3) * 1e9, \"shape\": (5, 3), \"name\": \"Large values (1e9)\"},\n",
    "        {\"data\": np.random.rand(1, 3) * 1e-9, \"shape\": (5, 3), \"name\": \"Small values (1e-9)\"},\n",
    "        {\"data\": np.array([[1e15], [1e-15]]), \"shape\": (2, 5), \"name\": \"Mixed extreme values\"},\n",
    "        \n",
    "        # Large dimensions\n",
    "        {\"data\": np.random.rand(1, 5), \"shape\": (1000, 5), \"name\": \"Broadcast to large first dim (1000)\"},\n",
    "        {\"data\": np.random.rand(5, 1), \"shape\": (5, 1000), \"name\": \"Broadcast to large second dim (1000)\"},\n",
    "        \n",
    "        # Higher dimensions\n",
    "        {\"data\": np.random.rand(1, 5, 1), \"shape\": (10, 5, 8), \"name\": \"3D broadcasting\"},\n",
    "        {\"data\": np.random.rand(1, 1, 1, 5), \"shape\": (7, 6, 5, 5), \"name\": \"4D broadcasting\"},\n",
    "        \n",
    "        # Multiple dimensions broadcasted\n",
    "        {\"data\": np.random.rand(1, 1, 3), \"shape\": (8, 8, 3), \"name\": \"Broadcasting multiple dimensions\"},\n",
    "        \n",
    "        # Special patterns\n",
    "        {\"data\": np.ones((1, 5)), \"shape\": (10, 5), \"name\": \"Broadcasting ones\"},\n",
    "        {\"data\": np.zeros((1, 5)), \"shape\": (10, 5), \"name\": \"Broadcasting zeros\"},\n",
    "        \n",
    "        # No broadcasting (identity case)\n",
    "        {\"data\": np.random.rand(5, 5), \"shape\": (5, 5), \"name\": \"No broadcasting (same shape)\"},\n",
    "        \n",
    "        # Edge cases\n",
    "        {\"data\": np.array([1.0]), \"shape\": (1, 1, 1, 1), \"name\": \"Scalar to higher dims\"},\n",
    "    ]\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        data = test_case[\"data\"]\n",
    "        shape = test_case[\"shape\"]\n",
    "        name = test_case[\"name\"]\n",
    "        \n",
    "        # Create tensors\n",
    "        pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "        x = Tensor(data, requires_grad=True)\n",
    "        \n",
    "        # PyTorch broadcast (expand)\n",
    "        # Handle the case of broadcasting to higher dimensions\n",
    "        if pt_x.dim() < len(shape):\n",
    "            # Add dimensions to match the target shape\n",
    "            expanded_dims = len(shape) - pt_x.dim()\n",
    "            reshape_dims = [1] * expanded_dims + list(pt_x.shape)\n",
    "            pt_x_reshaped = pt_x.reshape(reshape_dims)\n",
    "            expected = pt_x_reshaped.expand(shape)\n",
    "        else:\n",
    "            expected = pt_x.expand(shape)\n",
    "        \n",
    "        # Our broadcast_to\n",
    "        result = broadcast_to(x, shape)\n",
    "        \n",
    "        # Check forward pass\n",
    "        try:\n",
    "            np.testing.assert_allclose(\n",
    "                result.data,\n",
    "                expected.detach().numpy(),\n",
    "                rtol=1e-5, atol=1e-5,\n",
    "                err_msg=f\"broadcast_to forward pass failed\"\n",
    "            )\n",
    "            # print(f\"  ✓ Forward pass successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Forward pass failed: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Generate random gradient for backward pass\n",
    "        grad_output_np = np.random.rand(*shape).astype(np.float32)\n",
    "        grad_output_torch = torch.tensor(grad_output_np)\n",
    "        \n",
    "        # Compute gradients\n",
    "        expected.backward(grad_output_torch)\n",
    "        result.backward(grad_output_np)\n",
    "        \n",
    "        # Check backward pass\n",
    "        try:\n",
    "            np.testing.assert_allclose(\n",
    "                x.grad,\n",
    "                pt_x.grad.detach().numpy(),\n",
    "                rtol=1e-4, atol=1e-5,\n",
    "                err_msg=f\"broadcast_to backward pass failed\"\n",
    "            )\n",
    "            # print(f\"  ✓ Backward pass successful\")\n",
    "            result_msg = \"Successful.\"\n",
    "        except Exception as e:\n",
    "            result_msg = \"Failed.\"\n",
    "            print(f\"  ✗ Backward pass failed: {e}\")\n",
    "        \n",
    "        # Reset gradients\n",
    "        pt_x.grad = None\n",
    "        x.grad = np.zeros_like(x.data)\n",
    "        \n",
    "        print(\n",
    "            f\"Test case {i+1}: {name}.\"\n",
    "            # f\" Shape: {data.shape}, Axis: {axis}, Keepdims: {keepdims}.\"\n",
    "            f\" {result_msg}\"\n",
    "        )\n",
    "\n",
    "def test_softmax():\n",
    "    \"\"\"Test the softmax operation with challenging cases\"\"\"\n",
    "    print(\"\\n=== TESTING SOFTMAX ===\")\n",
    "    \n",
    "    # Define challenging test cases\n",
    "    test_cases = [\n",
    "        # Basic cases\n",
    "        {\"data\": np.random.rand(10), \"axis\": None, \"name\": \"Basic 1D vector\"},\n",
    "        {\"data\": np.random.rand(5, 5), \"axis\": 1, \"name\": \"Basic 2D, axis 1\"},\n",
    "        {\"data\": np.random.rand(5, 5), \"axis\": 0, \"name\": \"Basic 2D, axis 0\"},\n",
    "        \n",
    "        # Extreme values\n",
    "        {\"data\": np.random.rand(10) * 1e9, \"axis\": None, \"name\": \"Large values (1e9)\"},\n",
    "        {\"data\": np.random.rand(10) * 1e-9, \"axis\": None, \"name\": \"Small values (1e-9)\"},\n",
    "        {\"data\": np.array([1e15, 1e-15, 0, -1e-15, -1e15]), \"axis\": None, \"name\": \"Mixed extreme values\"},\n",
    "        \n",
    "        # Numerical stability challenges\n",
    "        {\"data\": np.array([1000, 0, -1000]), \"axis\": None, \"name\": \"Very different values\"},\n",
    "        {\"data\": np.array([1e5, 1e5 + 1e-5]), \"axis\": None, \"name\": \"Nearly identical large values\"},\n",
    "        {\"data\": np.ones(10) * 1e5, \"axis\": None, \"name\": \"All identical large values\"},\n",
    "        \n",
    "        # Large dimensions\n",
    "        {\"data\": np.random.rand(1000, 5), \"axis\": 1, \"name\": \"Large first dimension (1000x5)\"},\n",
    "        {\"data\": np.random.rand(5, 1000) * 1e9, \"axis\": 0, \"name\": \"Large second dimension (5x1000)\"},\n",
    "        \n",
    "        # Higher dimensions\n",
    "        {\"data\": np.random.rand(10, 10, 10), \"axis\": 2, \"name\": \"3D tensor, last axis\"},\n",
    "        {\"data\": np.random.rand(10, 10, 10), \"axis\": 1, \"name\": \"3D tensor, middle axis\"},\n",
    "        {\"data\": np.random.rand(5, 5, 5, 5), \"axis\": 0, \"name\": \"4D tensor, first axis\"},\n",
    "        \n",
    "        # Special patterns\n",
    "        {\"data\": np.zeros((10, 10)), \"axis\": 1, \"name\": \"All zeros (uniform distribution)\"},\n",
    "        {\"data\": np.ones((10, 10)), \"axis\": 1, \"name\": \"All ones (uniform distribution)\"},\n",
    "        {\"data\": np.eye(10), \"axis\": 1, \"name\": \"Identity matrix\"},\n",
    "        \n",
    "        # Edge cases\n",
    "        {\"data\": np.array([42.0]), \"axis\": None, \"name\": \"Single value (should be 1.0)\"},\n",
    "        {\"data\": np.zeros((1, 1, 1)), \"axis\": 1, \"name\": \"Multiple singleton dimensions\"},\n",
    "    ]\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        data = test_case[\"data\"]\n",
    "        axis = test_case[\"axis\"]\n",
    "        name = test_case[\"name\"]\n",
    "        \n",
    "        # print(f\"\\nTest case {i+1}: {name}\")\n",
    "        # print(f\"  Shape: {data.shape}, Axis: {axis}\")\n",
    "        \n",
    "        # Create tensors\n",
    "        pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "        x = Tensor(data, requires_grad=True)\n",
    "        \n",
    "        # PyTorch softmax\n",
    "        if axis is None:\n",
    "            # Flatten for axis=None\n",
    "            flattened = pt_x.reshape(-1)\n",
    "            expected = torch.nn.functional.softmax(flattened, dim=0)\n",
    "        else:\n",
    "            expected = torch.nn.functional.softmax(pt_x, dim=axis)\n",
    "        \n",
    "        # Our softmax\n",
    "        result = softmax(x, axis=axis)\n",
    "        \n",
    "        # Check forward pass\n",
    "        try:\n",
    "            np.testing.assert_allclose(\n",
    "                result.data,\n",
    "                expected.detach().numpy(),\n",
    "                rtol=1e-5, atol=1e-5,\n",
    "                err_msg=f\"Softmax forward pass failed\"\n",
    "            )\n",
    "            # print(f\"  ✓ Forward pass successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Forward pass failed: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Generate random gradient for backward pass\n",
    "        grad_output_np = np.random.rand(*result.data.shape).astype(np.float32)\n",
    "        \n",
    "        # For PyTorch, ensure gradient has the right shape\n",
    "        if axis is None:\n",
    "            grad_output_torch = torch.tensor(grad_output_np.reshape(-1))\n",
    "        else:\n",
    "            grad_output_torch = torch.tensor(grad_output_np)\n",
    "        \n",
    "        # Compute gradients\n",
    "        expected.backward(grad_output_torch)\n",
    "        result.backward(grad_output_np)\n",
    "        \n",
    "        # Check backward pass\n",
    "        try:\n",
    "            np.testing.assert_allclose(\n",
    "                x.grad,\n",
    "                pt_x.grad.detach().numpy(),\n",
    "                rtol=1e-4, atol=1e-5,\n",
    "                err_msg=f\"broadcast_to backward pass failed\"\n",
    "            )\n",
    "            # print(f\"  ✓ Backward pass successful\")\n",
    "            result_msg = \"Successful.\"\n",
    "        except Exception as e:\n",
    "            result_msg = \"Failed.\"\n",
    "            print(f\"  ✗ Backward pass failed: {e}\")\n",
    "        \n",
    "        # Reset gradients\n",
    "        pt_x.grad = None\n",
    "        x.grad = np.zeros_like(x.data)\n",
    "        \n",
    "        print(\n",
    "            f\"Test case {i+1}: {name}.\"\n",
    "            # f\" Shape: {data.shape}, Axis: {axis}, Keepdims: {keepdims}.\"\n",
    "            f\" {result_msg}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "6d2e2461-7f78-4bbe-aae9-6efd1a246e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TESTING SUMMATION ===\n",
      "Test case 1: Basic 2D, all axes. Successful.\n",
      "Test case 2: Basic 2D, axis 0. Successful.\n",
      "Test case 3: Basic 2D, axis 1 with keepdims. Successful.\n",
      "Test case 4: Large values (1e10). Successful.\n",
      "Test case 5: Small values (1e-10). Successful.\n",
      "Test case 6: Mixed extreme values. Successful.\n",
      "Test case 7: Large first dimension (1000x5). Successful.\n",
      "Test case 8: Large second dimension (5x1000). Successful.\n",
      "Test case 9: 3D with multiple axes. Successful.\n",
      "Test case 10: 4D with multiple axes and keepdims. Successful.\n",
      "Test case 11: All ones. Successful.\n",
      "Test case 12: All zeros. Successful.\n",
      "Test case 13: Identity matrix. Successful.\n",
      "Test case 14: Single value. Successful.\n",
      "Test case 15: Multiple singleton dimensions. Successful.\n",
      "\n",
      "=== TESTING BROADCAST_TO ===\n",
      "Test case 1: Scalar to vector. Successful.\n",
      "Test case 2: Row to matrix. Successful.\n",
      "Test case 3: Column to matrix. Successful.\n",
      "Test case 4: Large values (1e9). Successful.\n",
      "Test case 5: Small values (1e-9). Successful.\n",
      "Test case 6: Mixed extreme values. Successful.\n",
      "Test case 7: Broadcast to large first dim (1000). Successful.\n",
      "Test case 8: Broadcast to large second dim (1000). Successful.\n",
      "Test case 9: 3D broadcasting. Successful.\n",
      "Test case 10: 4D broadcasting. Successful.\n",
      "Test case 11: Broadcasting multiple dimensions. Successful.\n",
      "Test case 12: Broadcasting ones. Successful.\n",
      "Test case 13: Broadcasting zeros. Successful.\n",
      "Test case 14: No broadcasting (same shape). Successful.\n",
      "Test case 15: Scalar to higher dims. Successful.\n",
      "\n",
      "=== TESTING SOFTMAX ===\n",
      "Test case 1: Basic 1D vector. Successful.\n",
      "Test case 2: Basic 2D, axis 1. Successful.\n",
      "Test case 3: Basic 2D, axis 0. Successful.\n",
      "Test case 4: Large values (1e9). Successful.\n",
      "Test case 5: Small values (1e-9). Successful.\n",
      "Test case 6: Mixed extreme values. Successful.\n",
      "Test case 7: Very different values. Successful.\n",
      "  ✗ Forward pass failed: \n",
      "Not equal to tolerance rtol=1e-05, atol=1e-05\n",
      "Softmax forward pass failed\n",
      "Mismatched elements: 2 / 2 (100%)\n",
      "Max absolute difference among violations: 0.00108147\n",
      "Max relative difference among violations: 0.00216293\n",
      " ACTUAL: array([0.498919, 0.498919], dtype=float32)\n",
      " DESIRED: array([0.5, 0.5], dtype=float32)\n",
      "  ✗ Forward pass failed: \n",
      "Not equal to tolerance rtol=1e-05, atol=1e-05\n",
      "Softmax forward pass failed\n",
      "Mismatched elements: 10 / 10 (100%)\n",
      "Max absolute difference among violations: 0.00021002\n",
      "Max relative difference among violations: 0.00210017\n",
      " ACTUAL: array([0.09979, 0.09979, 0.09979, 0.09979, 0.09979, 0.09979, 0.09979,\n",
      "       0.09979, 0.09979, 0.09979], dtype=float32)\n",
      " DESIRED: array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], dtype=float32)\n",
      "Test case 10: Large first dimension (1000x5). Successful.\n",
      "Test case 11: Large second dimension (5x1000). Successful.\n",
      "Test case 12: 3D tensor, last axis. Successful.\n",
      "Test case 13: 3D tensor, middle axis. Successful.\n",
      "Test case 14: 4D tensor, first axis. Successful.\n",
      "Test case 15: All zeros (uniform distribution). Successful.\n",
      "Test case 16: All ones (uniform distribution). Successful.\n",
      "Test case 17: Identity matrix. Successful.\n",
      "Test case 18: Single value (should be 1.0). Successful.\n",
      "Test case 19: Multiple singleton dimensions. Successful.\n"
     ]
    }
   ],
   "source": [
    "test_summation()\n",
    "test_broadcast_to()\n",
    "test_softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb6c03-575f-45bd-92ba-5fa34460113e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f869ae6f-643c-46d8-8bbd-50724f8d9e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "90802025-08e1-45d1-b755-19a6234398dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.rand(4)\n",
    "np.expand_dims(Y.numpy(), axis=(0,2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "61da45ee-bfb7-485a-9f68-1f6d38ce0dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.482567 , 1.7950699, 1.6493765, 1.4997053]],\n",
       "\n",
       "       [[1.7428087, 1.6379938, 1.7570441, 1.3153229]]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logsumexp(X, dim=1, keepdims=True).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f55353db-eec2-45c5-96e2-af9662f753ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.482567 , 1.7950699, 1.6493764, 1.4997053]],\n",
       "\n",
       "       [[1.7428087, 1.6379938, 1.7570441, 1.3153229]]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = logsumexp(Tensor.from_torch(X), axis=1).data\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34f80aae-fce9-448e-a3af-36a1e31c0ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.482567 , 1.7950699, 1.6493764, 1.4997053],\n",
       "       [1.7428087, 1.6379938, 1.7570441, 1.3153229]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "570c273d-71e4-4f5c-8cdf-991fea14d927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3181, 0.8340, 0.0570, 0.2979],\n",
       "         [0.1563, 0.9888, 0.6898, 0.1264],\n",
       "         [0.6209, 0.0290, 0.7662, 0.6925]]),\n",
       " tensor([[0.8639, 0.0777, 0.8982, 0.2250],\n",
       "         [0.6347, 0.8138, 0.0857, 0.2969],\n",
       "         [0.3748, 0.5920, 0.8112, 0.1204]]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], X[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merger2",
   "language": "python",
   "name": "merger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
