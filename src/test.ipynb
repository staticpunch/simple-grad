{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad1a6fcd-1fda-4ef6-92cd-136a0a608394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77a7b90a-deb7-4ed2-840b-37cd375a8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e50e752b-1978-4172-8782-f30d821c2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import simplegrad as sg\n",
    "from simplegrad import Tensor\n",
    "from simplegrad import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6dc82f91-bebe-4fa7-a332-ff953a1194b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(tensor):\n",
    "    out = Tensor(np.log(tensor.data), requires_grad=tensor.requires_grad)\n",
    "    def _backward():\n",
    "        tensor.grad += out.grad / tensor.data\n",
    "    out._backward = _backward\n",
    "    out._prev = {tensor, }\n",
    "    return out\n",
    "    \n",
    "def logsumexp(tensor, axis=None, keepdims=False):\n",
    "    \"\"\"\n",
    "    mathematical operations, applied to 1D vector: \n",
    "    forward: log(e^z1 + e^z2 + ... + e^zn) = sum(e^zi)\n",
    "    backward: local_grad[i] = e^zi / sum(e^zi)\n",
    "    ------\n",
    "    for numerical stability:\n",
    "    forward: log(sum(e^zi))  = log(sum(e^(zi - zmax)) * e^zmax)\n",
    "                             = log(sum(e^(zi - zmax))) + log(e^zmax)\n",
    "    backward: e^zi/sum(e^zi) = e^(zi - zmax) / sum(e^(zi - zmax))\n",
    "    \"\"\"\n",
    "    max_z = np.max(tensor.data, axis=axis, keepdims=True)\n",
    "    stable_z = tensor.data - max_z\n",
    "    exp_stable_z = np.exp(stable_z)\n",
    "    stable_sum = np.sum(exp_stable_z, axis=axis, keepdims=keepdims)\n",
    "    max_term = max_z if keepdims else np.squeeze(max_z, axis=axis)\n",
    "    data = np.log(stable_sum) + max_term\n",
    "    out = Tensor(data, requires_grad=tensor.requires_grad)\n",
    "    \n",
    "    def _backward():\n",
    "        if tensor.requires_grad:\n",
    "            if axis is None:\n",
    "                # For None axis, basically all dims.\n",
    "                if not keepdims:\n",
    "                    grad_shaped = out.grad * np.ones_like(tensor.data)\n",
    "                    softmax_terms = exp_stable_z / np.sum(exp_stable_z)\n",
    "                    tensor.grad += grad_shaped * softmax_terms\n",
    "                else:\n",
    "                    # keepdims=True with axis=None\n",
    "                    softmax_terms = exp_stable_z / stable_sum\n",
    "                    tensor.grad += out.grad * softmax_terms\n",
    "            else:\n",
    "                # For specific axis reduction\n",
    "                grad_shaped = out.grad\n",
    "                if not keepdims:\n",
    "                    grad_shaped = np.expand_dims(grad_shaped, axis=axis)\n",
    "    \n",
    "                denom = stable_sum if keepdims \\\n",
    "                        else np.expand_dims(stable_sum, axis=axis)\n",
    "                softmax_terms = exp_stable_z / denom\n",
    "                tensor.grad += grad_shaped * softmax_terms\n",
    "    \n",
    "    out._backward = _backward\n",
    "    out._prev = {tensor, }\n",
    "    return out\n",
    "\n",
    "def softmax(tensor, axis: int = None):\n",
    "    \"\"\"\n",
    "    mathematical operations, applied to 1D vector:\n",
    "    forward: softmax(z)[i] = e^zi / sum(e^z)\n",
    "    backward: local_grad[i,j] = softmax(z)[i] * (1{i==j} - softmax(z)[j])\n",
    "    ------\n",
    "    for numerical stability. note: id(i, j) = 1{i == j}\n",
    "    forward: softmax(z)[i] = e^zi / sum(e^z)\n",
    "             = e^(zi) / e^(logsumexp(z))\n",
    "             = e^(zi - logsumexp(z))\n",
    "             \n",
    "    backward: local_grad[i,j] = softmax(z)[i] * (id(i,j) - softmax(z)[j])\n",
    "              When i=j: softmax(z)[i] * (1 - softmax(z)[i])\n",
    "              When iâ‰ j: -softmax(z)[i] * softmax(z)[j]  \n",
    "    \"\"\"\n",
    "    lse = logsumexp(tensor, axis=axis, keepdims=True)\n",
    "    out = Tensor(np.exp(tensor.data - lse.data), \n",
    "                 requires_grad=tensor.requires_grad)\n",
    "\n",
    "    def _backward():\n",
    "        if tensor.requires_grad:\n",
    "            tensor.grad += out.data * (out.grad - np.sum(out.grad * out.data, axis=axis, keepdims=True))\n",
    "            \n",
    "    out._backward = _backward\n",
    "    out._prev = {tensor, }\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a9d961d2-0829-4940-a7e5-bde3c2b2de8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tensor(2.485480785369873, requires_grad=True)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case = {\"data\": np.random.rand(3, 4) * 0.001, \"axis\": None, \"keepdims\": False}\n",
    "data = test_case[\"data\"]\n",
    "axis = test_case[\"axis\"]\n",
    "keepdims = test_case[\"keepdims\"]\n",
    "\n",
    "pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "x = Tensor.from_torch(pt_x)\n",
    "logsumexp(x, axis=axis, keepdims=keepdims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1f149e29-a0bb-4c19-a273-ed82f658b60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LogSumExp operation...\n",
      "LogSumExp forward test 1 passed!\n",
      "LogSumExp backward test 1 passed!\n",
      "LogSumExp forward test 2 passed!\n",
      "LogSumExp backward test 2 passed!\n",
      "LogSumExp forward test 3 passed!\n",
      "LogSumExp backward test 3 passed!\n",
      "LogSumExp forward test 4 passed!\n",
      "LogSumExp backward test 4 passed!\n",
      "LogSumExp forward test 5 passed!\n",
      "LogSumExp backward test 5 passed!\n",
      "LogSumExp forward test 6 passed!\n",
      "LogSumExp backward test 6 passed!\n",
      "LogSumExp forward test 7 passed!\n",
      "LogSumExp backward test 7 passed!\n",
      "LogSumExp forward test 8 passed!\n",
      "LogSumExp backward test 8 passed!\n",
      "LogSumExp forward test 9 passed!\n",
      "LogSumExp backward test 9 passed!\n",
      "LogSumExp forward test 10 passed!\n",
      "LogSumExp backward test 10 passed!\n",
      "\n",
      "Testing LogSumExp specific cases...\n",
      "LogSumExp specific case 1 (large identical values) passed!\n",
      "LogSumExp specific case 2 (extreme value differences) passed!\n",
      "LogSumExp specific case 3 (softmax relation) passed!\n",
      "\n",
      "All LogSumExp tests completed successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing LogSumExp operation...\")\n",
    "test_logsumexp()\n",
    "\n",
    "print(\"\\nTesting LogSumExp specific cases...\")\n",
    "test_logsumexp_specific_cases()\n",
    "\n",
    "print(\"\\nAll LogSumExp tests completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f13b87f7-09e1-4273-b2a6-866e286a9b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from simplegrad.tensor import Tensor\n",
    "\n",
    "\n",
    "def test_logsumexp():\n",
    "    # Define diverse test cases\n",
    "    test_cases = [\n",
    "        # Small values\n",
    "        {\"data\": np.random.rand(3, 4) * 0.001, \"axis\": None, \"keepdims\": False},\n",
    "        # Large values\n",
    "        {\"data\": np.random.rand(3, 4) * 100, \"axis\": None, \"keepdims\": False},\n",
    "        # Negative values\n",
    "        {\"data\": np.random.rand(3, 4) * -10, \"axis\": None, \"keepdims\": False},\n",
    "        # Mixed values\n",
    "        {\"data\": np.random.rand(3, 4) * 2 - 1, \"axis\": None, \"keepdims\": False},\n",
    "        # Single dimension reduction with keepdims=True\n",
    "        {\"data\": np.random.rand(3, 4) * 2 - 1, \"axis\": 0, \"keepdims\": True},\n",
    "        # Single dimension reduction with keepdims=False\n",
    "        {\"data\": np.random.rand(3, 4) * 2 - 1, \"axis\": 1, \"keepdims\": False},\n",
    "        # Multiple dimensions\n",
    "        {\"data\": np.random.rand(2, 3, 4) * 2 - 1, \"axis\": None, \"keepdims\": False},\n",
    "        # Multiple dimensions with specific axis\n",
    "        {\"data\": np.random.rand(2, 3, 4) * 2 - 1, \"axis\": 1, \"keepdims\": False},\n",
    "        # Multiple dimensions with tuple axis\n",
    "        {\"data\": np.random.rand(2, 3, 4) * 2 - 1, \"axis\": (0, 2), \"keepdims\": False},\n",
    "        # Multiple dimensions with tuple axis and keepdims=True\n",
    "        {\"data\": np.random.rand(2, 3, 4) * 2 - 1, \"axis\": (0, 2), \"keepdims\": True}\n",
    "    ]\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        data = test_case[\"data\"]\n",
    "        axis = test_case[\"axis\"]\n",
    "        keepdims = test_case[\"keepdims\"]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "        x = Tensor.from_torch(pt_x)\n",
    "        \n",
    "        # PyTorch version\n",
    "        if axis is None:\n",
    "            # PyTorch's logsumexp requires a specific dim\n",
    "            expected = torch.logsumexp(pt_x, dim=tuple(range(pt_x.dim())), keepdim=keepdims)\n",
    "        elif isinstance(axis, int):\n",
    "            expected = torch.logsumexp(pt_x, dim=axis, keepdim=keepdims)\n",
    "        else:\n",
    "            # For multiple axes, we need to handle them one by one in PyTorch\n",
    "            temp = pt_x\n",
    "            # Process axes in reverse order to maintain correct dimensions\n",
    "            for ax in sorted(axis, reverse=True):\n",
    "                temp = torch.logsumexp(temp, dim=ax, keepdim=keepdims)\n",
    "            expected = temp\n",
    "        \n",
    "        # Our implementation\n",
    "        result = logsumexp(x, axis=axis, keepdims=keepdims)\n",
    "        \n",
    "        # Check forward pass\n",
    "        np.testing.assert_allclose(\n",
    "            result.data, \n",
    "            expected.detach().numpy(), \n",
    "            rtol=1e-5, atol=1e-5,\n",
    "            err_msg=f\"Forward pass failed for test case {i+1}: data shape {data.shape}, axis {axis}, keepdims {keepdims}\"\n",
    "        )\n",
    "        print(f\"LogSumExp forward test {i+1} passed!\")\n",
    "        \n",
    "        # Compute gradients\n",
    "        grad_output = torch.ones_like(expected)\n",
    "        expected.backward(grad_output)\n",
    "        result.backward()\n",
    "        \n",
    "        # Check backward pass\n",
    "        np.testing.assert_allclose(\n",
    "            x.grad, \n",
    "            pt_x.grad.detach().numpy(), \n",
    "            rtol=1e-5, atol=1e-5,\n",
    "            err_msg=f\"Backward pass failed for test case {i+1}: data shape {data.shape}, axis {axis}, keepdims {keepdims}\"\n",
    "        )\n",
    "        print(f\"LogSumExp backward test {i+1} passed!\")\n",
    "\n",
    "\n",
    "def test_logsumexp_specific_cases():\n",
    "    \"\"\"Test specific edge cases for logsumexp\"\"\"\n",
    "    \n",
    "    # Case 1: All elements are the same (tests numerical stability)\n",
    "    data = np.ones((3, 3)) * 1000  # Large identical values\n",
    "    pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "    x = Tensor.from_torch(pt_x)\n",
    "    \n",
    "    expected = torch.logsumexp(pt_x, dim=1, keepdim=False)\n",
    "    result = logsumexp(x, axis=1, keepdims=False)\n",
    "    \n",
    "    np.testing.assert_allclose(result.data, expected.detach().numpy(), rtol=1e-5, atol=1e-5)\n",
    "    print(\"LogSumExp specific case 1 (large identical values) passed!\")\n",
    "    \n",
    "    # Case 2: Extreme differences between values (tests numerical stability)\n",
    "    data = np.array([[1e-10, 1e10], [1e-10, 1e-10]])\n",
    "    pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "    x = Tensor.from_torch(pt_x)\n",
    "    \n",
    "    expected = torch.logsumexp(pt_x, dim=1, keepdim=False)\n",
    "    result = logsumexp(x, axis=1, keepdims=False)\n",
    "    \n",
    "    np.testing.assert_allclose(result.data, expected.detach().numpy(), rtol=1e-5, atol=1e-5)\n",
    "    print(\"LogSumExp specific case 2 (extreme value differences) passed!\")\n",
    "    \n",
    "    # Case 3: Test with softmax relation (logsumexp is used in softmax implementation)\n",
    "    data = np.random.rand(5, 10) * 2 - 1\n",
    "    pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "    x = Tensor.from_torch(pt_x)\n",
    "    \n",
    "    # Standard softmax calculation using logsumexp\n",
    "    pt_logsumexp = torch.logsumexp(pt_x, dim=1, keepdim=True)\n",
    "    pt_softmax = torch.exp(pt_x - pt_logsumexp)\n",
    "    \n",
    "    our_logsumexp = logsumexp(x, axis=1, keepdims=True)\n",
    "    our_softmax = np.exp(x.data - our_logsumexp.data)\n",
    "    \n",
    "    np.testing.assert_allclose(our_softmax, pt_softmax.detach().numpy(), rtol=1e-5, atol=1e-5)\n",
    "    print(\"LogSumExp specific case 3 (softmax relation) passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6d2e2461-7f78-4bbe-aae9-6efd1a246e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogSumExp forward test passed!\n",
      "LogSumExp backward test passed!\n"
     ]
    }
   ],
   "source": [
    "test_logsumexp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f869ae6f-643c-46d8-8bbd-50724f8d9e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "90802025-08e1-45d1-b755-19a6234398dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = torch.rand(4)\n",
    "np.expand_dims(Y.numpy(), axis=(0,2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "61da45ee-bfb7-485a-9f68-1f6d38ce0dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.482567 , 1.7950699, 1.6493765, 1.4997053]],\n",
       "\n",
       "       [[1.7428087, 1.6379938, 1.7570441, 1.3153229]]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logsumexp(X, dim=1, keepdims=True).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f55353db-eec2-45c5-96e2-af9662f753ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.482567 , 1.7950699, 1.6493764, 1.4997053]],\n",
       "\n",
       "       [[1.7428087, 1.6379938, 1.7570441, 1.3153229]]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = logsumexp(Tensor.from_torch(X), axis=1).data\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34f80aae-fce9-448e-a3af-36a1e31c0ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.482567 , 1.7950699, 1.6493764, 1.4997053],\n",
       "       [1.7428087, 1.6379938, 1.7570441, 1.3153229]], dtype=float32)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.squeeze(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "570c273d-71e4-4f5c-8cdf-991fea14d927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3181, 0.8340, 0.0570, 0.2979],\n",
       "         [0.1563, 0.9888, 0.6898, 0.1264],\n",
       "         [0.6209, 0.0290, 0.7662, 0.6925]]),\n",
       " tensor([[0.8639, 0.0777, 0.8982, 0.2250],\n",
       "         [0.6347, 0.8138, 0.0857, 0.2969],\n",
       "         [0.3748, 0.5920, 0.8112, 0.1204]]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], X[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merger2",
   "language": "python",
   "name": "merger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
