{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad1a6fcd-1fda-4ef6-92cd-136a0a608394",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a7b90a-deb7-4ed2-840b-37cd375a8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e50e752b-1978-4172-8782-f30d821c2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import simplegrad as sg\n",
    "import torch\n",
    "from simplegrad import Tensor, ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8148d6-40e4-4b9f-a875-374a548acb79",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Develop Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dc82f91-bebe-4fa7-a332-ff953a1194b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(tensor):\n",
    "    out = Tensor(np.log(tensor.data), requires_grad=tensor.requires_grad)\n",
    "\n",
    "    def _backward():\n",
    "        tensor.grad += out.grad / tensor.data\n",
    "\n",
    "    out._backward = _backward\n",
    "    out._prev = {\n",
    "        tensor,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "def exp(tensor):\n",
    "    out = Tensor(np.exp(tensor.data), requires_grad=tensor.requires_grad)\n",
    "\n",
    "    def _backward():\n",
    "        if tensor.requires_grad:\n",
    "            tensor.grad += out.data * out.grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    out._prev = {\n",
    "        tensor,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "def summation(tensor, axis=None, keepdims=False):\n",
    "    \"\"\"\n",
    "    local_grad = d.sum(x) / d.xi = 1.\n",
    "    therefore derivative of x is basically just out.grad\n",
    "    broadcasted to the shape of the input tensor.\n",
    "    \"\"\"\n",
    "    out = Tensor(\n",
    "        np.sum(tensor.data, axis=axis, keepdims=keepdims),\n",
    "        requires_grad=tensor.requires_grad,\n",
    "    )\n",
    "\n",
    "    def _backward():\n",
    "        if tensor.requires_grad:\n",
    "            input_shape, axes = tensor.data.shape, axis\n",
    "\n",
    "            if not keepdims:\n",
    "                if axis is None:  # if self.axes is None, take sum over all axes.\n",
    "                    axes = tuple(i for i in range(len(input_shape)))\n",
    "                elif isinstance(axis, int):\n",
    "                    axes = (axis,)\n",
    "\n",
    "                shape_range = range(len(input_shape))\n",
    "                mask = np.array([0 if i in axes else 1 for i in shape_range])\n",
    "                new_shape = np.array(input_shape) * mask + (1 - mask)\n",
    "                grad = np.reshape(out.grad, new_shape)\n",
    "                grad = np.broadcast_to(grad, input_shape)\n",
    "            else:\n",
    "                grad = np.broadcast_to(out.grad, input_shape)\n",
    "\n",
    "            tensor.grad += grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    out._prev = {\n",
    "        tensor,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "def broadcast_to(tensor, shape):\n",
    "    \"\"\"this is interestingly the reverse of summation.\"\"\"\n",
    "    if tensor.shape == shape:  # Optimization: no-op if shapes match\n",
    "        return tensor\n",
    "\n",
    "    out_data = np.broadcast_to(tensor.data, shape)\n",
    "    out = Tensor(out_data, requires_grad=tensor.requires_grad)\n",
    "\n",
    "    input_shape = tensor.shape  # Capture input shape for backward pass\n",
    "\n",
    "    def _backward():\n",
    "        if tensor.requires_grad:\n",
    "            ishape, oshape = tensor.data.shape, out.grad.shape\n",
    "            ## in = (3, 1, 4), out = (3, 5, 4) -> aligned = (3, 1, 4)\n",
    "            ## i think numpy only implicitly broadcast to prefix dims :/\n",
    "            aligned = [1] * (len(oshape) - len(ishape)) + list(ishape)\n",
    "            broadcast_axes = tuple([i for i, axis in enumerate(aligned) if axis == 1])\n",
    "            grad = np.sum(out.grad, axis=broadcast_axes, keepdims=True)\n",
    "            grad = np.reshape(grad, ishape)\n",
    "\n",
    "            tensor.grad += grad\n",
    "\n",
    "    out._backward = _backward\n",
    "    out._prev = {\n",
    "        tensor,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "def logsumexp(tensor, axis=None, keepdims=False):\n",
    "    \"\"\"\n",
    "    mathematical operations, applied to 1D vector:\n",
    "    forward: log(e^z1 + e^z2 + ... + e^zn) = sum(e^zi)\n",
    "    backward: local_grad[i] = e^zi / sum(e^zi)\n",
    "    ------\n",
    "    for numerical stability:\n",
    "    forward: log(sum(e^zi))  = log(sum(e^(zi - zmax)) * e^zmax)\n",
    "                             = log(sum(e^(zi - zmax))) + log(e^zmax)\n",
    "    backward: e^zi/sum(e^zi) = e^(zi - zmax) / sum(e^(zi - zmax))\n",
    "    \"\"\"\n",
    "    max_z = np.max(tensor.data, axis=axis, keepdims=True)\n",
    "    stable_z = tensor.data - max_z\n",
    "    exp_stable_z = np.exp(stable_z)\n",
    "    stable_sum = np.sum(exp_stable_z, axis=axis, keepdims=keepdims)\n",
    "    max_term = max_z if keepdims else np.squeeze(max_z, axis=axis)\n",
    "    data = np.log(stable_sum) + max_term\n",
    "    out = Tensor(data, requires_grad=tensor.requires_grad)\n",
    "\n",
    "    def _backward():\n",
    "        if tensor.requires_grad:\n",
    "            if axis is None:\n",
    "                # For None axis, basically all dims.\n",
    "                if not keepdims:\n",
    "                    grad_shaped = out.grad * np.ones_like(tensor.data)\n",
    "                    softmax_terms = exp_stable_z / np.sum(exp_stable_z)\n",
    "                    tensor.grad += grad_shaped * softmax_terms\n",
    "                else:\n",
    "                    # keepdims=True with axis=None\n",
    "                    softmax_terms = exp_stable_z / stable_sum\n",
    "                    tensor.grad += out.grad * softmax_terms\n",
    "            else:\n",
    "                # For specific axis reduction\n",
    "                grad_shaped = out.grad\n",
    "                if not keepdims:\n",
    "                    grad_shaped = np.expand_dims(grad_shaped, axis=axis)\n",
    "\n",
    "                denom = (\n",
    "                    stable_sum if keepdims else np.expand_dims(stable_sum, axis=axis)\n",
    "                )\n",
    "                softmax_terms = exp_stable_z / denom\n",
    "                tensor.grad += grad_shaped * softmax_terms\n",
    "\n",
    "    out._backward = _backward\n",
    "    out._prev = {\n",
    "        tensor,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    1. note: id(i, j) = 1{i == j}\n",
    "    mathematical operations, applied to 1D vector:\n",
    "    \n",
    "    forward: softmax(z)[i] = e^zi / sum(e^z)\n",
    "    backward: since softmax is a vector-to-vector function,\n",
    "              the local_grad we need to compute is a Jacobian:\n",
    "        local_grad[i,j] = softmax(z)[i] * (id(i,j) - softmax(z)[j])\n",
    "    \n",
    "    \n",
    "    2. for numerical stability. \n",
    "    forward: softmax(z)[i] = e^zi / sum(e^z)\n",
    "             = e^(zi) / e^(logsumexp(z))\n",
    "             = e^(zi - logsumexp(z))\n",
    "             \n",
    "    backward (vectorized): \n",
    "        let ID.shape = local_grad.shape = (N, N). \n",
    "            ID[i, j] = id(i, j).\n",
    "            local_grad[i, j] = d.s[i] / d.z[j]\n",
    "            \n",
    "        local_grad[i, j] = out[i] * ID[i, j] - out[i] * out[j]\n",
    "        local_grad[i, :] = out[i] * ID[i, :] - out[i] * out[:]\n",
    "        local_grad[:, :] = out[:] * ID[:, :] - out[:, None] * out[None, :]\n",
    "                         = diag(out) - outer(out, out)\n",
    "\n",
    "        shit, however, this is not really efficient.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def softmax(tensor, axis: int = None):\n",
    "    \"\"\"\n",
    "    to reduce headache, actually I should implement an exp ops,\n",
    "    then let the chain rule do its job automatically.\n",
    "    \"\"\"\n",
    "    lse = logsumexp(tensor, axis=axis, keepdims=True)\n",
    "    # print(tensor.shape, lse.shape)\n",
    "    lse_broadcast = broadcast_to(lse, tensor.data.shape)\n",
    "    log_softmax = tensor - lse_broadcast\n",
    "    out = exp(log_softmax)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f13b87f7-09e1-4273-b2a6-866e286a9b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from simplegrad.tensor import Tensor\n",
    "\n",
    "\n",
    "def test_logsumexp():\n",
    "    # Define diverse test cases\n",
    "    test_cases = [\n",
    "        # Small values\n",
    "        {\"data\": np.random.rand(3, 4) * 0.001, \"axis\": None, \"keepdims\": False},\n",
    "        # Large values\n",
    "        {\"data\": np.random.rand(3, 4) * 100, \"axis\": None, \"keepdims\": False},\n",
    "        # Negative values\n",
    "        {\"data\": np.random.rand(3, 4) * -10, \"axis\": None, \"keepdims\": False},\n",
    "        # Mixed values\n",
    "        {\"data\": np.random.rand(3, 4) * 2 - 1, \"axis\": None, \"keepdims\": False},\n",
    "        # Single dimension reduction with keepdims=True\n",
    "        {\"data\": np.random.rand(3, 4) * 2 - 1, \"axis\": 0, \"keepdims\": True},\n",
    "        # Single dimension reduction with keepdims=False\n",
    "        {\"data\": np.random.rand(3, 4) * 2 - 1, \"axis\": 1, \"keepdims\": False},\n",
    "        # Multiple dimensions\n",
    "        {\"data\": np.random.rand(2, 3, 4) * 2 - 1, \"axis\": None, \"keepdims\": False},\n",
    "        # Multiple dimensions with specific axis\n",
    "        {\"data\": np.random.rand(2, 3, 4) * 2 - 1, \"axis\": 1, \"keepdims\": False},\n",
    "        # Multiple dimensions with tuple axis\n",
    "        {\"data\": np.random.rand(2, 3, 4) * 2 - 1, \"axis\": (0, 2), \"keepdims\": False},\n",
    "        # Multiple dimensions with tuple axis and keepdims=True\n",
    "        {\"data\": np.random.rand(2, 3, 4) * 2 - 1, \"axis\": (0, 2), \"keepdims\": True},\n",
    "    ]\n",
    "\n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        data = test_case[\"data\"]\n",
    "        axis = test_case[\"axis\"]\n",
    "        keepdims = test_case[\"keepdims\"]\n",
    "\n",
    "        # Convert to tensors\n",
    "        pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "        x = Tensor.from_torch(pt_x)\n",
    "\n",
    "        # PyTorch version\n",
    "        if axis is None:\n",
    "            # PyTorch's logsumexp requires a specific dim\n",
    "            expected = torch.logsumexp(\n",
    "                pt_x, dim=tuple(range(pt_x.dim())), keepdim=keepdims\n",
    "            )\n",
    "        elif isinstance(axis, int):\n",
    "            expected = torch.logsumexp(pt_x, dim=axis, keepdim=keepdims)\n",
    "        else:\n",
    "            # For multiple axes, we need to handle them one by one in PyTorch\n",
    "            temp = pt_x\n",
    "            # Process axes in reverse order to maintain correct dimensions\n",
    "            for ax in sorted(axis, reverse=True):\n",
    "                temp = torch.logsumexp(temp, dim=ax, keepdim=keepdims)\n",
    "            expected = temp\n",
    "\n",
    "        # Our implementation\n",
    "        result = logsumexp(x, axis=axis, keepdims=keepdims)\n",
    "\n",
    "        # Check forward pass\n",
    "        np.testing.assert_allclose(\n",
    "            result.data,\n",
    "            expected.detach().numpy(),\n",
    "            rtol=1e-5,\n",
    "            atol=1e-5,\n",
    "            err_msg=f\"Forward pass failed for test case {i+1}: data shape {data.shape}, axis {axis}, keepdims {keepdims}\",\n",
    "        )\n",
    "        print(f\"LogSumExp forward test {i+1} passed!\")\n",
    "\n",
    "        # Compute gradients\n",
    "        grad_output = torch.ones_like(expected)\n",
    "        expected.backward(grad_output)\n",
    "        result.backward()\n",
    "\n",
    "        # Check backward pass\n",
    "        np.testing.assert_allclose(\n",
    "            x.grad,\n",
    "            pt_x.grad.detach().numpy(),\n",
    "            rtol=1e-5,\n",
    "            atol=1e-5,\n",
    "            err_msg=f\"Backward pass failed for test case {i+1}: data shape {data.shape}, axis {axis}, keepdims {keepdims}\",\n",
    "        )\n",
    "        print(f\"LogSumExp backward test {i+1} passed!\")\n",
    "\n",
    "\n",
    "def test_logsumexp_specific_cases():\n",
    "    \"\"\"Test specific edge cases for logsumexp\"\"\"\n",
    "\n",
    "    # Case 1: All elements are the same (tests numerical stability)\n",
    "    data = np.ones((3, 3)) * 1000  # Large identical values\n",
    "    pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "    x = Tensor.from_torch(pt_x)\n",
    "\n",
    "    expected = torch.logsumexp(pt_x, dim=1, keepdim=False)\n",
    "    result = logsumexp(x, axis=1, keepdims=False)\n",
    "\n",
    "    np.testing.assert_allclose(\n",
    "        result.data, expected.detach().numpy(), rtol=1e-5, atol=1e-5\n",
    "    )\n",
    "    print(\"LogSumExp specific case 1 (large identical values) passed!\")\n",
    "\n",
    "    # Case 2: Extreme differences between values (tests numerical stability)\n",
    "    data = np.array([[1e-10, 1e10], [1e-10, 1e-10]])\n",
    "    pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "    x = Tensor.from_torch(pt_x)\n",
    "\n",
    "    expected = torch.logsumexp(pt_x, dim=1, keepdim=False)\n",
    "    result = logsumexp(x, axis=1, keepdims=False)\n",
    "\n",
    "    np.testing.assert_allclose(\n",
    "        result.data, expected.detach().numpy(), rtol=1e-5, atol=1e-5\n",
    "    )\n",
    "    print(\"LogSumExp specific case 2 (extreme value differences) passed!\")\n",
    "\n",
    "    # Case 3: Test with softmax relation (logsumexp is used in softmax implementation)\n",
    "    data = np.random.rand(5, 10) * 2 - 1\n",
    "    pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "    x = Tensor.from_torch(pt_x)\n",
    "\n",
    "    # Standard softmax calculation using logsumexp\n",
    "    pt_logsumexp = torch.logsumexp(pt_x, dim=1, keepdim=True)\n",
    "    pt_softmax = torch.exp(pt_x - pt_logsumexp)\n",
    "\n",
    "    our_logsumexp = logsumexp(x, axis=1, keepdims=True)\n",
    "    our_softmax = np.exp(x.data - our_logsumexp.data)\n",
    "\n",
    "    np.testing.assert_allclose(\n",
    "        our_softmax, pt_softmax.detach().numpy(), rtol=1e-5, atol=1e-5\n",
    "    )\n",
    "    print(\"LogSumExp specific case 3 (softmax relation) passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f149e29-a0bb-4c19-a273-ed82f658b60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LogSumExp operation...\n",
      "LogSumExp forward test 1 passed!\n",
      "LogSumExp backward test 1 passed!\n",
      "LogSumExp forward test 2 passed!\n",
      "LogSumExp backward test 2 passed!\n",
      "LogSumExp forward test 3 passed!\n",
      "LogSumExp backward test 3 passed!\n",
      "LogSumExp forward test 4 passed!\n",
      "LogSumExp backward test 4 passed!\n",
      "LogSumExp forward test 5 passed!\n",
      "LogSumExp backward test 5 passed!\n",
      "LogSumExp forward test 6 passed!\n",
      "LogSumExp backward test 6 passed!\n",
      "LogSumExp forward test 7 passed!\n",
      "LogSumExp backward test 7 passed!\n",
      "LogSumExp forward test 8 passed!\n",
      "LogSumExp backward test 8 passed!\n",
      "LogSumExp forward test 9 passed!\n",
      "LogSumExp backward test 9 passed!\n",
      "LogSumExp forward test 10 passed!\n",
      "LogSumExp backward test 10 passed!\n",
      "\n",
      "Testing LogSumExp specific cases...\n",
      "LogSumExp specific case 1 (large identical values) passed!\n",
      "LogSumExp specific case 2 (extreme value differences) passed!\n",
      "LogSumExp specific case 3 (softmax relation) passed!\n",
      "\n",
      "All LogSumExp tests completed successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing LogSumExp operation...\")\n",
    "test_logsumexp()\n",
    "\n",
    "print(\"\\nTesting LogSumExp specific cases...\")\n",
    "test_logsumexp_specific_cases()\n",
    "\n",
    "print(\"\\nAll LogSumExp tests completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6487915f-6905-4d74-bdab-b604e586f2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_summation():\n",
    "    \"\"\"Test the summation operation with challenging cases\"\"\"\n",
    "    print(\"\\n=== TESTING SUMMATION ===\")\n",
    "\n",
    "    # Define challenging test cases\n",
    "    test_cases = [\n",
    "        # Basic cases\n",
    "        {\n",
    "            \"data\": np.random.rand(5, 5),\n",
    "            \"axis\": None,\n",
    "            \"keepdims\": False,\n",
    "            \"name\": \"Basic 2D, all axes\",\n",
    "        },\n",
    "        {\n",
    "            \"data\": np.random.rand(5, 5),\n",
    "            \"axis\": 0,\n",
    "            \"keepdims\": False,\n",
    "            \"name\": \"Basic 2D, axis 0\",\n",
    "        },\n",
    "        {\n",
    "            \"data\": np.random.rand(5, 5),\n",
    "            \"axis\": 1,\n",
    "            \"keepdims\": True,\n",
    "            \"name\": \"Basic 2D, axis 1 with keepdims\",\n",
    "        },\n",
    "        # Extreme values\n",
    "        {\n",
    "            \"data\": np.random.rand(10, 10) * 1e10,\n",
    "            \"axis\": None,\n",
    "            \"keepdims\": False,\n",
    "            \"name\": \"Large values (1e10)\",\n",
    "        },\n",
    "        {\n",
    "            \"data\": np.random.rand(10, 10) * 1e-10,\n",
    "            \"axis\": 0,\n",
    "            \"keepdims\": True,\n",
    "            \"name\": \"Small values (1e-10)\",\n",
    "        },\n",
    "        {\n",
    "            \"data\": np.array([[1e15, 1e-15], [1e-15, 1e15]]),\n",
    "            \"axis\": 1,\n",
    "            \"keepdims\": False,\n",
    "            \"name\": \"Mixed extreme values\",\n",
    "        },\n",
    "        # Large dimensions\n",
    "        {\n",
    "            \"data\": np.random.rand(1000, 5),\n",
    "            \"axis\": 0,\n",
    "            \"keepdims\": False,\n",
    "            \"name\": \"Large first dimension (1000x5)\",\n",
    "        },\n",
    "        {\n",
    "            \"data\": np.random.rand(5, 1000),\n",
    "            \"axis\": 1,\n",
    "            \"keepdims\": True,\n",
    "            \"name\": \"Large second dimension (5x1000)\",\n",
    "        },\n",
    "        # Higher dimensions\n",
    "        {\n",
    "            \"data\": np.random.rand(10, 10, 10),\n",
    "            \"axis\": (0, 2),\n",
    "            \"keepdims\": False,\n",
    "            \"name\": \"3D with multiple axes\",\n",
    "        },\n",
    "        {\n",
    "            \"data\": np.random.rand(5, 5, 5, 5),\n",
    "            \"axis\": (1, 2),\n",
    "            \"keepdims\": True,\n",
    "            \"name\": \"4D with multiple axes and keepdims\",\n",
    "        },\n",
    "        # Special patterns\n",
    "        {\n",
    "            \"data\": np.ones((20, 20)),\n",
    "            \"axis\": None,\n",
    "            \"keepdims\": False,\n",
    "            \"name\": \"All ones\",\n",
    "        },\n",
    "        {\"data\": np.zeros((20, 20)), \"axis\": 0, \"keepdims\": True, \"name\": \"All zeros\"},\n",
    "        {\"data\": np.eye(20), \"axis\": 1, \"keepdims\": False, \"name\": \"Identity matrix\"},\n",
    "        # Edge cases\n",
    "        {\n",
    "            \"data\": np.array([1.0]),\n",
    "            \"axis\": None,\n",
    "            \"keepdims\": False,\n",
    "            \"name\": \"Single value\",\n",
    "        },\n",
    "        {\n",
    "            \"data\": np.random.rand(1, 1, 1, 1),\n",
    "            \"axis\": (1, 2),\n",
    "            \"keepdims\": True,\n",
    "            \"name\": \"Multiple singleton dimensions\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        data = test_case[\"data\"]\n",
    "        axis = test_case[\"axis\"]\n",
    "        keepdims = test_case[\"keepdims\"]\n",
    "        name = test_case[\"name\"]\n",
    "\n",
    "        # Create tensors\n",
    "        pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "        x = Tensor(data, requires_grad=True)\n",
    "\n",
    "        # PyTorch sum\n",
    "        if isinstance(axis, tuple):\n",
    "            # For multiple axes in PyTorch, we need to do them one by one\n",
    "            expected = pt_x\n",
    "            for ax in sorted(axis, reverse=True):  # Start from the highest axis\n",
    "                expected = expected.sum(dim=ax, keepdim=keepdims)\n",
    "        else:\n",
    "            expected = pt_x.sum(dim=axis, keepdim=keepdims)\n",
    "\n",
    "        # Our summation\n",
    "        result = summation(x, axis=axis, keepdims=keepdims)\n",
    "\n",
    "        # Check forward pass\n",
    "        try:\n",
    "            np.testing.assert_allclose(\n",
    "                result.data,\n",
    "                expected.detach().numpy(),\n",
    "                rtol=1e-5,\n",
    "                atol=1e-5,\n",
    "                err_msg=f\"Summation forward pass failed\",\n",
    "            )\n",
    "            # print(f\"  ✓ Forward pass successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Forward pass failed: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Generate random gradient for backward pass\n",
    "        grad_output_np = np.random.rand(*result.data.shape)\n",
    "        if isinstance(grad_output_np, float):\n",
    "            grad_output_np = np.float32(grad_output_np)\n",
    "        else:\n",
    "            grad_output_np = grad_output_np.astype(np.float32)\n",
    "        grad_output_torch = torch.tensor(grad_output_np)\n",
    "\n",
    "        # Compute gradients\n",
    "        expected.backward(grad_output_torch)\n",
    "        result.backward(grad_output_np)\n",
    "\n",
    "        # Check backward pass\n",
    "        try:\n",
    "            np.testing.assert_allclose(\n",
    "                x.grad,\n",
    "                pt_x.grad.detach().numpy(),\n",
    "                rtol=1e-4,\n",
    "                atol=1e-5,\n",
    "                err_msg=f\"broadcast_to backward pass failed\",\n",
    "            )\n",
    "            # print(f\"  ✓ Backward pass successful\")\n",
    "            result_msg = \"Successful.\"\n",
    "        except Exception as e:\n",
    "            result_msg = \"Failed.\"\n",
    "            print(f\"  ✗ Backward pass failed: {e}\")\n",
    "\n",
    "        # Reset gradients\n",
    "        pt_x.grad = None\n",
    "        x.grad = np.zeros_like(x.data)\n",
    "\n",
    "        print(\n",
    "            f\"Test case {i+1}: {name}.\"\n",
    "            # f\" Shape: {data.shape}, Axis: {axis}, Keepdims: {keepdims}.\"\n",
    "            f\" {result_msg}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c178a3d2-69a5-4e6f-9407-fab9b7d6349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_broadcast_to():\n",
    "    \"\"\"Test the broadcast_to operation with challenging cases\"\"\"\n",
    "    print(\"\\n=== TESTING BROADCAST_TO ===\")\n",
    "\n",
    "    # Define challenging test cases\n",
    "    test_cases = [\n",
    "        # Basic broadcasting\n",
    "        {\"data\": np.random.rand(1), \"shape\": (10,), \"name\": \"Scalar to vector\"},\n",
    "        {\"data\": np.random.rand(1, 5), \"shape\": (10, 5), \"name\": \"Row to matrix\"},\n",
    "        {\"data\": np.random.rand(5, 1), \"shape\": (5, 10), \"name\": \"Column to matrix\"},\n",
    "        # Extreme values\n",
    "        {\n",
    "            \"data\": np.random.rand(1, 3) * 1e9,\n",
    "            \"shape\": (5, 3),\n",
    "            \"name\": \"Large values (1e9)\",\n",
    "        },\n",
    "        {\n",
    "            \"data\": np.random.rand(1, 3) * 1e-9,\n",
    "            \"shape\": (5, 3),\n",
    "            \"name\": \"Small values (1e-9)\",\n",
    "        },\n",
    "        {\n",
    "            \"data\": np.array([[1e15], [1e-15]]),\n",
    "            \"shape\": (2, 5),\n",
    "            \"name\": \"Mixed extreme values\",\n",
    "        },\n",
    "        # Large dimensions\n",
    "        {\n",
    "            \"data\": np.random.rand(1, 5),\n",
    "            \"shape\": (1000, 5),\n",
    "            \"name\": \"Broadcast to large first dim (1000)\",\n",
    "        },\n",
    "        {\n",
    "            \"data\": np.random.rand(5, 1),\n",
    "            \"shape\": (5, 1000),\n",
    "            \"name\": \"Broadcast to large second dim (1000)\",\n",
    "        },\n",
    "        # Higher dimensions\n",
    "        {\n",
    "            \"data\": np.random.rand(1, 5, 1),\n",
    "            \"shape\": (10, 5, 8),\n",
    "            \"name\": \"3D broadcasting\",\n",
    "        },\n",
    "        {\n",
    "            \"data\": np.random.rand(1, 1, 1, 5),\n",
    "            \"shape\": (7, 6, 5, 5),\n",
    "            \"name\": \"4D broadcasting\",\n",
    "        },\n",
    "        # Multiple dimensions broadcasted\n",
    "        {\n",
    "            \"data\": np.random.rand(1, 1, 3),\n",
    "            \"shape\": (8, 8, 3),\n",
    "            \"name\": \"Broadcasting multiple dimensions\",\n",
    "        },\n",
    "        # Special patterns\n",
    "        {\"data\": np.ones((1, 5)), \"shape\": (10, 5), \"name\": \"Broadcasting ones\"},\n",
    "        {\"data\": np.zeros((1, 5)), \"shape\": (10, 5), \"name\": \"Broadcasting zeros\"},\n",
    "        # No broadcasting (identity case)\n",
    "        {\n",
    "            \"data\": np.random.rand(5, 5),\n",
    "            \"shape\": (5, 5),\n",
    "            \"name\": \"No broadcasting (same shape)\",\n",
    "        },\n",
    "        # Edge cases\n",
    "        {\n",
    "            \"data\": np.array([1.0]),\n",
    "            \"shape\": (1, 1, 1, 1),\n",
    "            \"name\": \"Scalar to higher dims\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        data = test_case[\"data\"]\n",
    "        shape = test_case[\"shape\"]\n",
    "        name = test_case[\"name\"]\n",
    "\n",
    "        # Create tensors\n",
    "        pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "        x = Tensor(data, requires_grad=True)\n",
    "\n",
    "        # PyTorch broadcast (expand)\n",
    "        # Handle the case of broadcasting to higher dimensions\n",
    "        if pt_x.dim() < len(shape):\n",
    "            # Add dimensions to match the target shape\n",
    "            expanded_dims = len(shape) - pt_x.dim()\n",
    "            reshape_dims = [1] * expanded_dims + list(pt_x.shape)\n",
    "            pt_x_reshaped = pt_x.reshape(reshape_dims)\n",
    "            expected = pt_x_reshaped.expand(shape)\n",
    "        else:\n",
    "            expected = pt_x.expand(shape)\n",
    "\n",
    "        # Our broadcast_to\n",
    "        result = broadcast_to(x, shape)\n",
    "\n",
    "        # Check forward pass\n",
    "        try:\n",
    "            np.testing.assert_allclose(\n",
    "                result.data,\n",
    "                expected.detach().numpy(),\n",
    "                rtol=1e-5,\n",
    "                atol=1e-5,\n",
    "                err_msg=f\"broadcast_to forward pass failed\",\n",
    "            )\n",
    "            # print(f\"  ✓ Forward pass successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Forward pass failed: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Generate random gradient for backward pass\n",
    "        grad_output_np = np.random.rand(*shape).astype(np.float32)\n",
    "        grad_output_torch = torch.tensor(grad_output_np)\n",
    "\n",
    "        # Compute gradients\n",
    "        expected.backward(grad_output_torch)\n",
    "        result.backward(grad_output_np)\n",
    "\n",
    "        # Check backward pass\n",
    "        try:\n",
    "            np.testing.assert_allclose(\n",
    "                x.grad,\n",
    "                pt_x.grad.detach().numpy(),\n",
    "                rtol=1e-4,\n",
    "                atol=1e-5,\n",
    "                err_msg=f\"broadcast_to backward pass failed\",\n",
    "            )\n",
    "            # print(f\"  ✓ Backward pass successful\")\n",
    "            result_msg = \"Successful.\"\n",
    "        except Exception as e:\n",
    "            result_msg = \"Failed.\"\n",
    "            print(f\"  ✗ Backward pass failed: {e}\")\n",
    "\n",
    "        # Reset gradients\n",
    "        pt_x.grad = None\n",
    "        x.grad = np.zeros_like(x.data)\n",
    "\n",
    "        print(\n",
    "            f\"Test case {i+1}: {name}.\"\n",
    "            # f\" Shape: {data.shape}, Axis: {axis}, Keepdims: {keepdims}.\"\n",
    "            f\" {result_msg}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "148f4315-c4dd-4394-8826-3d54c2eeb809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_softmax():\n",
    "    \"\"\"Test the softmax operation with challenging cases\"\"\"\n",
    "    print(\"\\n=== TESTING SOFTMAX ===\")\n",
    "\n",
    "    # Define challenging test cases\n",
    "    test_cases = [\n",
    "        # Basic cases\n",
    "        {\"data\": np.random.rand(10), \"axis\": None, \"name\": \"Basic 1D vector\"},\n",
    "        {\"data\": np.random.rand(5, 5), \"axis\": 1, \"name\": \"Basic 2D, axis 1\"},\n",
    "        {\"data\": np.random.rand(5, 5), \"axis\": 0, \"name\": \"Basic 2D, axis 0\"},\n",
    "        # Extreme values\n",
    "        {\"data\": np.random.rand(10) * 1e9, \"axis\": None, \"name\": \"Large values (1e9)\"},\n",
    "        {\n",
    "            \"data\": np.random.rand(10) * 1e-9,\n",
    "            \"axis\": None,\n",
    "            \"name\": \"Small values (1e-9)\",\n",
    "        },\n",
    "        {\n",
    "            \"data\": np.array([1e15, 1e-15, 0, -1e-15, -1e15]),\n",
    "            \"axis\": None,\n",
    "            \"name\": \"Mixed extreme values\",\n",
    "        },\n",
    "        # Numerical stability challenges\n",
    "        {\n",
    "            \"data\": np.array([1000, 0, -1000]),\n",
    "            \"axis\": None,\n",
    "            \"name\": \"Very different values\",\n",
    "        },\n",
    "        {\n",
    "            \"data\": np.array([1e5, 1e5 + 1e-5]),\n",
    "            \"axis\": None,\n",
    "            \"name\": \"Nearly identical large values\",\n",
    "        },\n",
    "        {\"data\": np.ones(10) * 1e5, \"axis\": None, \"name\": \"All identical large values\"},\n",
    "        # Large dimensions\n",
    "        {\n",
    "            \"data\": np.random.rand(1000, 5),\n",
    "            \"axis\": 1,\n",
    "            \"name\": \"Large first dimension (1000x5)\",\n",
    "        },\n",
    "        {\n",
    "            \"data\": np.random.rand(5, 1000) * 1e9,\n",
    "            \"axis\": 0,\n",
    "            \"name\": \"Large second dimension (5x1000)\",\n",
    "        },\n",
    "        # Higher dimensions\n",
    "        {\"data\": np.random.rand(10, 10, 10), \"axis\": 2, \"name\": \"3D tensor, last axis\"},\n",
    "        {\n",
    "            \"data\": np.random.rand(10, 10, 10),\n",
    "            \"axis\": 1,\n",
    "            \"name\": \"3D tensor, middle axis\",\n",
    "        },\n",
    "        {\n",
    "            \"data\": np.random.rand(5, 5, 5, 5),\n",
    "            \"axis\": 0,\n",
    "            \"name\": \"4D tensor, first axis\",\n",
    "        },\n",
    "        # Special patterns\n",
    "        {\n",
    "            \"data\": np.zeros((10, 10)),\n",
    "            \"axis\": 1,\n",
    "            \"name\": \"All zeros (uniform distribution)\",\n",
    "        },\n",
    "        {\n",
    "            \"data\": np.ones((10, 10)),\n",
    "            \"axis\": 1,\n",
    "            \"name\": \"All ones (uniform distribution)\",\n",
    "        },\n",
    "        {\"data\": np.eye(10), \"axis\": 1, \"name\": \"Identity matrix\"},\n",
    "        # Edge cases\n",
    "        {\n",
    "            \"data\": np.array([42.0]),\n",
    "            \"axis\": None,\n",
    "            \"name\": \"Single value (should be 1.0)\",\n",
    "        },\n",
    "        {\n",
    "            \"data\": np.zeros((1, 1, 1)),\n",
    "            \"axis\": 1,\n",
    "            \"name\": \"Multiple singleton dimensions\",\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        data = test_case[\"data\"]\n",
    "        axis = test_case[\"axis\"]\n",
    "        name = test_case[\"name\"]\n",
    "\n",
    "        # print(f\"\\nTest case {i+1}: {name}\")\n",
    "        # print(f\"  Shape: {data.shape}, Axis: {axis}\")\n",
    "\n",
    "        # Create tensors\n",
    "        pt_x = torch.tensor(data, dtype=torch.float32, requires_grad=True)\n",
    "        x = Tensor(data, requires_grad=True)\n",
    "\n",
    "        # PyTorch softmax\n",
    "        if axis is None:\n",
    "            # Flatten for axis=None\n",
    "            flattened = pt_x.reshape(-1)\n",
    "            expected = torch.nn.functional.softmax(flattened, dim=0)\n",
    "        else:\n",
    "            expected = torch.nn.functional.softmax(pt_x, dim=axis)\n",
    "\n",
    "        # Our softmax\n",
    "        result = softmax(x, axis=axis)\n",
    "\n",
    "        # Check forward pass\n",
    "        try:\n",
    "            np.testing.assert_allclose(\n",
    "                result.data,\n",
    "                expected.detach().numpy(),\n",
    "                rtol=1e-5,\n",
    "                atol=1e-5,\n",
    "                err_msg=f\"Softmax forward pass failed\",\n",
    "            )\n",
    "            # print(f\"  ✓ Forward pass successful\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Forward pass failed: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Generate random gradient for backward pass\n",
    "        grad_output_np = np.random.rand(*result.data.shape).astype(np.float32)\n",
    "\n",
    "        # For PyTorch, ensure gradient has the right shape\n",
    "        if axis is None:\n",
    "            grad_output_torch = torch.tensor(grad_output_np.reshape(-1))\n",
    "        else:\n",
    "            grad_output_torch = torch.tensor(grad_output_np)\n",
    "\n",
    "        # Compute gradients\n",
    "        expected.backward(grad_output_torch)\n",
    "        result.backward(grad_output_np)\n",
    "\n",
    "        # Check backward pass\n",
    "        try:\n",
    "            np.testing.assert_allclose(\n",
    "                x.grad,\n",
    "                pt_x.grad.detach().numpy(),\n",
    "                rtol=1e-4,\n",
    "                atol=1e-5,\n",
    "                err_msg=f\"broadcast_to backward pass failed\",\n",
    "            )\n",
    "            # print(f\"  ✓ Backward pass successful\")\n",
    "            result_msg = \"Successful.\"\n",
    "        except Exception as e:\n",
    "            result_msg = \"Failed.\"\n",
    "            print(f\"  ✗ Backward pass failed: {e}\")\n",
    "\n",
    "        # Reset gradients\n",
    "        pt_x.grad = None\n",
    "        x.grad = np.zeros_like(x.data)\n",
    "\n",
    "        print(\n",
    "            f\"Test case {i+1}: {name}.\"\n",
    "            # f\" Shape: {data.shape}, Axis: {axis}, Keepdims: {keepdims}.\"\n",
    "            f\" {result_msg}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d2e2461-7f78-4bbe-aae9-6efd1a246e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TESTING SUMMATION ===\n",
      "Test case 1: Basic 2D, all axes. Successful.\n",
      "Test case 2: Basic 2D, axis 0. Successful.\n",
      "Test case 3: Basic 2D, axis 1 with keepdims. Successful.\n",
      "Test case 4: Large values (1e10). Successful.\n",
      "Test case 5: Small values (1e-10). Successful.\n",
      "Test case 6: Mixed extreme values. Successful.\n",
      "Test case 7: Large first dimension (1000x5). Successful.\n",
      "Test case 8: Large second dimension (5x1000). Successful.\n",
      "Test case 9: 3D with multiple axes. Successful.\n",
      "Test case 10: 4D with multiple axes and keepdims. Successful.\n",
      "Test case 11: All ones. Successful.\n",
      "Test case 12: All zeros. Successful.\n",
      "Test case 13: Identity matrix. Successful.\n",
      "Test case 14: Single value. Successful.\n",
      "Test case 15: Multiple singleton dimensions. Successful.\n",
      "\n",
      "=== TESTING BROADCAST_TO ===\n",
      "Test case 1: Scalar to vector. Successful.\n",
      "Test case 2: Row to matrix. Successful.\n",
      "Test case 3: Column to matrix. Successful.\n",
      "Test case 4: Large values (1e9). Successful.\n",
      "Test case 5: Small values (1e-9). Successful.\n",
      "Test case 6: Mixed extreme values. Successful.\n",
      "Test case 7: Broadcast to large first dim (1000). Successful.\n",
      "Test case 8: Broadcast to large second dim (1000). Successful.\n",
      "Test case 9: 3D broadcasting. Successful.\n",
      "Test case 10: 4D broadcasting. Successful.\n",
      "Test case 11: Broadcasting multiple dimensions. Successful.\n",
      "Test case 12: Broadcasting ones. Successful.\n",
      "Test case 13: Broadcasting zeros. Successful.\n",
      "Test case 14: No broadcasting (same shape). Successful.\n",
      "Test case 15: Scalar to higher dims. Successful.\n",
      "\n",
      "=== TESTING SOFTMAX ===\n",
      "Test case 1: Basic 1D vector. Successful.\n",
      "Test case 2: Basic 2D, axis 1. Successful.\n",
      "Test case 3: Basic 2D, axis 0. Successful.\n",
      "Test case 4: Large values (1e9). Successful.\n",
      "Test case 5: Small values (1e-9). Successful.\n",
      "Test case 6: Mixed extreme values. Successful.\n",
      "Test case 7: Very different values. Successful.\n",
      "  ✗ Forward pass failed: \n",
      "Not equal to tolerance rtol=1e-05, atol=1e-05\n",
      "Softmax forward pass failed\n",
      "Mismatched elements: 2 / 2 (100%)\n",
      "Max absolute difference among violations: 0.00108147\n",
      "Max relative difference among violations: 0.00216293\n",
      " ACTUAL: array([0.498919, 0.498919], dtype=float32)\n",
      " DESIRED: array([0.5, 0.5], dtype=float32)\n",
      "  ✗ Forward pass failed: \n",
      "Not equal to tolerance rtol=1e-05, atol=1e-05\n",
      "Softmax forward pass failed\n",
      "Mismatched elements: 10 / 10 (100%)\n",
      "Max absolute difference among violations: 0.00021002\n",
      "Max relative difference among violations: 0.00210017\n",
      " ACTUAL: array([0.09979, 0.09979, 0.09979, 0.09979, 0.09979, 0.09979, 0.09979,\n",
      "       0.09979, 0.09979, 0.09979], dtype=float32)\n",
      " DESIRED: array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1], dtype=float32)\n",
      "Test case 10: Large first dimension (1000x5). Successful.\n",
      "Test case 11: Large second dimension (5x1000). Successful.\n",
      "Test case 12: 3D tensor, last axis. Successful.\n",
      "Test case 13: 3D tensor, middle axis. Successful.\n",
      "Test case 14: 4D tensor, first axis. Successful.\n",
      "Test case 15: All zeros (uniform distribution). Successful.\n",
      "Test case 16: All ones (uniform distribution). Successful.\n",
      "Test case 17: Identity matrix. Successful.\n",
      "Test case 18: Single value (should be 1.0). Successful.\n",
      "Test case 19: Multiple singleton dimensions. Successful.\n"
     ]
    }
   ],
   "source": [
    "test_summation()\n",
    "test_broadcast_to()\n",
    "test_softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089aed3a-3420-4ab1-bbdf-5c5281637dbb",
   "metadata": {},
   "source": [
    "## Develop optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3ce0c25b-8f42-4407-a78a-e75cf4c4a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e010880a-cfa6-42ff-b1c9-69a63b01ad5f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Reference training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74156e7c-e747-419b-a677-eda9e023d343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecimalToBase4NN(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_digits=10, input_classes=10, hidden_size=128, output_digits=6\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A neural network to convert decimal numbers to base-4 representation.\n",
    "\n",
    "        Args:\n",
    "            input_digits: Number of decimal digits to encode\n",
    "            input_classes: Number of possible values for each input digit (10 for decimal)\n",
    "            hidden_size: Size of hidden layer\n",
    "            output_digits: Number of base-4 digits to predict\n",
    "        \"\"\"\n",
    "        super(DecimalToBase4NN, self).__init__()\n",
    "        self.input_digits = input_digits\n",
    "        self.output_digits = output_digits\n",
    "\n",
    "        # Calculate total input size (one-hot vectors for each digit)\n",
    "        input_size = input_digits * input_classes\n",
    "\n",
    "        # Network architecture: linear -> relu -> linear -> softmax (implicitly during loss calculation)\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(\n",
    "                hidden_size, output_digits * 4\n",
    "            ),  # 4 possible values (0,1,2,3) for each output digit\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        batch_size = x.size(0)\n",
    "        output = self.model(x)\n",
    "\n",
    "        # Reshape to [batch_size, output_digits, 4] for softmax across the 4 possible values\n",
    "        output = output.view(batch_size, self.output_digits, 4)\n",
    "\n",
    "        # Apply softmax to get probabilities for each digit\n",
    "        output = torch.softmax(output, dim=2)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def decimal_to_digits(decimal_num, num_digits=10):\n",
    "    \"\"\"Convert a decimal number to its individual digits (right-padded with zeros).\"\"\"\n",
    "    digits = []\n",
    "    temp = decimal_num\n",
    "\n",
    "    # Extract individual digits\n",
    "    while temp > 0:\n",
    "        digits.append(temp % 10)\n",
    "        temp //= 10\n",
    "\n",
    "    # Pad with zeros to reach num_digits\n",
    "    while len(digits) < num_digits:\n",
    "        digits.append(0)\n",
    "\n",
    "    # Reverse since we calculated least significant digit first\n",
    "    return digits[::-1]\n",
    "\n",
    "\n",
    "def decimal_to_base4(decimal_num, max_digits=6):\n",
    "    \"\"\"Convert a decimal number to its base-4 representation.\"\"\"\n",
    "    if decimal_num == 0:\n",
    "        return [0] * max_digits\n",
    "\n",
    "    base4_digits = []\n",
    "    temp = decimal_num\n",
    "\n",
    "    while temp > 0:\n",
    "        base4_digits.append(temp % 4)\n",
    "        temp //= 4\n",
    "\n",
    "    # Pad with zeros to reach max_digits\n",
    "    while len(base4_digits) < max_digits:\n",
    "        base4_digits.append(0)\n",
    "\n",
    "    # Reverse since we calculated least significant digit first\n",
    "    return base4_digits[::-1][-max_digits:]\n",
    "\n",
    "\n",
    "def generate_training_data(\n",
    "    num_samples=10000, max_decimal=4 ** 6 - 1, input_digits=10, output_digits=6\n",
    "):\n",
    "    \"\"\"Generate training data: pairs of (one-hot encoded decimal digits, base-4 representation).\"\"\"\n",
    "    # Create a list of all possible numbers from 0 to max_decimal\n",
    "    all_possible_nums = np.arange(max_decimal + 1)\n",
    "\n",
    "    # Sample from the complete list\n",
    "    if num_samples <= len(all_possible_nums):\n",
    "        decimal_nums = np.random.choice(\n",
    "            all_possible_nums, size=num_samples, replace=False\n",
    "        )\n",
    "    else:\n",
    "        # If num_samples is larger than possible numbers, use all numbers\n",
    "        print(\n",
    "            f\"Warning: Requested {num_samples} samples but only {len(all_possible_nums)} numbers exist in the range.\"\n",
    "        )\n",
    "        print(f\"Using all {len(all_possible_nums)} numbers.\")\n",
    "        decimal_nums = all_possible_nums\n",
    "        np.random.shuffle(decimal_nums)  # Shuffle to ensure random order\n",
    "\n",
    "    # Convert each decimal to its individual digits\n",
    "    X_digits = [decimal_to_digits(num, input_digits) for num in decimal_nums]\n",
    "\n",
    "    # One-hot encode the input decimal digits\n",
    "    X = torch.zeros(num_samples, input_digits, 10)\n",
    "    for i, dec_digits in enumerate(X_digits):\n",
    "        for j, digit in enumerate(dec_digits):\n",
    "            X[i, j, digit] = 1.0\n",
    "\n",
    "    # Flatten the one-hot vectors\n",
    "    X = X.view(num_samples, -1)\n",
    "\n",
    "    # Convert each decimal to its base-4 representation\n",
    "    y_list = [decimal_to_base4(num, output_digits) for num in decimal_nums]\n",
    "\n",
    "    # One-hot encode the base-4 digits\n",
    "    y = torch.zeros(num_samples, output_digits, 4)\n",
    "    for i, base4_num in enumerate(y_list):\n",
    "        for j, digit in enumerate(base4_num):\n",
    "            y[i, j, digit] = 1.0\n",
    "\n",
    "    return X, y, decimal_nums\n",
    "\n",
    "\n",
    "def train_test_split_data(X, y, decimal_nums, test_size=0.2):\n",
    "    \"\"\"Split data into training and testing sets.\"\"\"\n",
    "    dataset = TensorDataset(X, y)\n",
    "\n",
    "    # Calculate sizes\n",
    "    dataset_size = len(dataset)\n",
    "    test_size = int(test_size * dataset_size)\n",
    "    train_size = dataset_size - test_size\n",
    "\n",
    "    # Create tensor of indices and then split it\n",
    "    all_indices = torch.arange(dataset_size)\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Randomly shuffle the indices\n",
    "    shuffled_indices = all_indices[torch.randperm(dataset_size)]\n",
    "\n",
    "    # Split the shuffled indices\n",
    "    train_indices = shuffled_indices[:train_size]\n",
    "    test_indices = shuffled_indices[train_size:]\n",
    "\n",
    "    # Create dataset subsets\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "    # Safely split the decimal numbers to match the X and y tensors\n",
    "    # Ensure we're using the correct indices within range of decimal_nums\n",
    "    safe_train_indices = train_indices[train_indices < len(decimal_nums)]\n",
    "    safe_test_indices = test_indices[test_indices < len(decimal_nums)]\n",
    "\n",
    "    if len(safe_train_indices) < len(train_indices) or len(safe_test_indices) < len(\n",
    "        test_indices\n",
    "    ):\n",
    "        print(\n",
    "            f\"Warning: decimal_nums has {len(decimal_nums)} elements, but dataset has {dataset_size} elements.\"\n",
    "        )\n",
    "        print(f\"Using only the valid indices for decimal_nums.\")\n",
    "\n",
    "    # If decimal_nums is shorter than X/y (which can happen after the change to generate_training_data),\n",
    "    # we need to pad it or truncate the indices\n",
    "    if len(decimal_nums) < dataset_size:\n",
    "        # Pad decimal_nums with zeros to match dataset size\n",
    "        padded_nums = np.zeros(dataset_size, dtype=decimal_nums.dtype)\n",
    "        padded_nums[: len(decimal_nums)] = decimal_nums\n",
    "        train_decimal_nums = padded_nums[train_indices.numpy()]\n",
    "        test_decimal_nums = padded_nums[test_indices.numpy()]\n",
    "    else:\n",
    "        # Use original array as-is\n",
    "        train_decimal_nums = decimal_nums[train_indices.numpy()]\n",
    "        test_decimal_nums = decimal_nums[test_indices.numpy()]\n",
    "\n",
    "    return train_dataset, test_dataset, train_decimal_nums, test_decimal_nums\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    device,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    learning_rate=0.01,\n",
    "    log_steps=50,\n",
    "):\n",
    "    \"\"\"Train the neural network using SGD optimizer.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # Move batch to device\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(batch_X)\n",
    "\n",
    "            # Reshape for cross entropy loss\n",
    "            batch_size = batch_X.size(0)\n",
    "            predictions = predictions.view(batch_size * model.output_digits, 4)\n",
    "            targets = batch_y.view(batch_size * model.output_digits, 4)\n",
    "            targets = torch.argmax(targets, dim=1)  # Convert one-hot to class indices\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(predictions, targets)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if (epoch + 1) % log_steps == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_dataset, device):\n",
    "    \"\"\"Evaluate the model on the test dataset.\"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    test_loader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "    correct_digits = 0\n",
    "    total_digits = 0\n",
    "    correct_numbers = 0\n",
    "    total_numbers = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            # Move batch to device\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(batch_X)\n",
    "\n",
    "            # Get predicted digits\n",
    "            pred_digits = torch.argmax(predictions, dim=2)\n",
    "            true_digits = torch.argmax(batch_y, dim=2)\n",
    "\n",
    "            # Count correct digits\n",
    "            correct_digits += (pred_digits == true_digits).sum().item()\n",
    "            total_digits += pred_digits.numel()\n",
    "\n",
    "            # Count correct numbers (all digits correct)\n",
    "            correct_per_sample = (pred_digits == true_digits).all(dim=1)\n",
    "            correct_numbers += correct_per_sample.sum().item()\n",
    "            total_numbers += correct_per_sample.numel()\n",
    "\n",
    "    digit_accuracy = correct_digits / total_digits * 100\n",
    "    number_accuracy = correct_numbers / total_numbers * 100\n",
    "\n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"Digit-level accuracy: {digit_accuracy:.2f}%\")\n",
    "    print(f\"Number-level accuracy: {number_accuracy:.2f}%\")\n",
    "\n",
    "    return digit_accuracy, number_accuracy\n",
    "\n",
    "\n",
    "def test_specific_examples(\n",
    "    model, test_decimal_nums, device, input_digits=10, output_digits=6, num_samples=20\n",
    "):\n",
    "    \"\"\"Test the model on specific examples from the test set.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Randomly select samples from test set\n",
    "    if num_samples > len(test_decimal_nums):\n",
    "        num_samples = len(test_decimal_nums)\n",
    "\n",
    "    indices = np.random.choice(len(test_decimal_nums), num_samples, replace=False)\n",
    "    sample_nums = test_decimal_nums[indices]\n",
    "\n",
    "    # Convert to one-hot encoded digits\n",
    "    X_digits = [decimal_to_digits(num, input_digits) for num in sample_nums]\n",
    "    X_test = torch.zeros(num_samples, input_digits, 10)\n",
    "    for i, dec_digits in enumerate(X_digits):\n",
    "        for j, digit in enumerate(dec_digits):\n",
    "            X_test[i, j, digit] = 1.0\n",
    "    X_test = X_test.view(num_samples, -1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test)\n",
    "\n",
    "    print(\"\\nTesting specific examples from the test set:\")\n",
    "    for i, decimal_num in enumerate(sample_nums):\n",
    "        # Get the predicted base-4 digits\n",
    "        pred_digits = torch.argmax(predictions[i], dim=1).cpu().numpy()\n",
    "\n",
    "        # Get the true base-4 digits\n",
    "        true_digits = decimal_to_base4(decimal_num, output_digits)\n",
    "        true_digits = np.array([int(x) for x in true_digits])\n",
    "\n",
    "        # Convert predictions to a single number for easy comparison\n",
    "        pred_decimal = sum(\n",
    "            digit * (4 ** (output_digits - idx - 1))\n",
    "            for idx, digit in enumerate(pred_digits)\n",
    "        )\n",
    "\n",
    "        is_correct = np.array_equal(pred_digits, true_digits)\n",
    "        print(\n",
    "            f\"Decimal: {decimal_num}. Correct: {is_correct}\"\n",
    "            f\"\\n    Predicted Base-4: {pred_digits}\"\n",
    "            f\"\\n    True Base-4     : {true_digits}\"\n",
    "        )\n",
    "\n",
    "        if not is_correct:\n",
    "            print(f\"    Predicted as decimal: {pred_decimal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "903d5fe8-9ee3-4362-881e-6adbab8ad09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:1\n",
      "Using 16384 samples (max possible: 16384)\n",
      "Training samples: 14746\n",
      "Testing samples: 1638\n",
      "Initial model:\n",
      "\n",
      "Test Results:\n",
      "Digit-level accuracy: 24.59%\n",
      "Number-level accuracy: 0.00%\n",
      "Epoch 50/2000, Loss: 1.1575\n",
      "Epoch 100/2000, Loss: 1.1124\n",
      "Epoch 150/2000, Loss: 1.0928\n",
      "Epoch 200/2000, Loss: 1.0848\n",
      "Epoch 250/2000, Loss: 1.0742\n",
      "Epoch 300/2000, Loss: 1.0683\n",
      "Epoch 350/2000, Loss: 1.0651\n",
      "Epoch 400/2000, Loss: 1.0624\n",
      "Epoch 450/2000, Loss: 1.0595\n",
      "Epoch 500/2000, Loss: 1.0563\n",
      "Epoch 550/2000, Loss: 1.0528\n",
      "Epoch 600/2000, Loss: 1.0476\n",
      "Epoch 650/2000, Loss: 1.0433\n",
      "Epoch 700/2000, Loss: 1.0392\n",
      "Epoch 750/2000, Loss: 1.0343\n",
      "Epoch 800/2000, Loss: 1.0270\n",
      "Epoch 850/2000, Loss: 1.0154\n",
      "Epoch 900/2000, Loss: 0.9999\n",
      "Epoch 950/2000, Loss: 0.9837\n",
      "Epoch 1000/2000, Loss: 0.9715\n",
      "Epoch 1050/2000, Loss: 0.9632\n",
      "Epoch 1100/2000, Loss: 0.9578\n",
      "Epoch 1150/2000, Loss: 0.9541\n",
      "Epoch 1200/2000, Loss: 0.9513\n",
      "Epoch 1250/2000, Loss: 0.9494\n",
      "Epoch 1300/2000, Loss: 0.9478\n",
      "Epoch 1350/2000, Loss: 0.9466\n",
      "Epoch 1400/2000, Loss: 0.9456\n",
      "Epoch 1450/2000, Loss: 0.9446\n",
      "Epoch 1500/2000, Loss: 0.9437\n",
      "Epoch 1550/2000, Loss: 0.9431\n",
      "Epoch 1600/2000, Loss: 0.9426\n",
      "Epoch 1650/2000, Loss: 0.9420\n",
      "Epoch 1700/2000, Loss: 0.9415\n",
      "Epoch 1750/2000, Loss: 0.9410\n",
      "Epoch 1800/2000, Loss: 0.9407\n",
      "Epoch 1850/2000, Loss: 0.9402\n",
      "Epoch 1900/2000, Loss: 0.9400\n",
      "Epoch 1950/2000, Loss: 0.9395\n",
      "Epoch 2000/2000, Loss: 0.9393\n",
      "\n",
      "Test Results:\n",
      "Digit-level accuracy: 80.60%\n",
      "Number-level accuracy: 21.25%\n",
      "\n",
      "Testing specific examples from the test set:\n",
      "Decimal: 8400. Correct: False\n",
      "    Predicted Base-4: [2 0 1 2 1 0 0]\n",
      "    True Base-4     : [2 0 0 3 1 0 0]\n",
      "    Predicted as decimal: 8592\n",
      "Decimal: 9793. Correct: False\n",
      "    Predicted Base-4: [2 1 2 2 0 2 3]\n",
      "    True Base-4     : [2 1 2 1 0 0 1]\n",
      "    Predicted as decimal: 9867\n",
      "Decimal: 8091. Correct: False\n",
      "    Predicted Base-4: [2 3 3 2 1 2 3]\n",
      "    True Base-4     : [1 3 3 2 1 2 3]\n",
      "    Predicted as decimal: 12187\n",
      "Decimal: 10411. Correct: False\n",
      "    Predicted Base-4: [2 2 1 2 2 2 3]\n",
      "    True Base-4     : [2 2 0 2 2 2 3]\n",
      "    Predicted as decimal: 10667\n",
      "Decimal: 16124. Correct: False\n",
      "    Predicted Base-4: [3 3 3 3 3 1 0]\n",
      "    True Base-4     : [3 3 2 3 3 3 0]\n",
      "    Predicted as decimal: 16372\n",
      "Decimal: 10698. Correct: False\n",
      "    Predicted Base-4: [2 2 1 2 0 2 2]\n",
      "    True Base-4     : [2 2 1 3 0 2 2]\n",
      "    Predicted as decimal: 10634\n",
      "Decimal: 14822. Correct: False\n",
      "    Predicted Base-4: [3 2 1 3 2 1 0]\n",
      "    True Base-4     : [3 2 1 3 2 1 2]\n",
      "    Predicted as decimal: 14820\n",
      "Decimal: 7470. Correct: False\n",
      "    Predicted Base-4: [1 3 1 0 3 1 2]\n",
      "    True Base-4     : [1 3 1 0 2 3 2]\n",
      "    Predicted as decimal: 7478\n",
      "Decimal: 3882. Correct: False\n",
      "    Predicted Base-4: [0 3 2 0 2 2 0]\n",
      "    True Base-4     : [0 3 3 0 2 2 2]\n",
      "    Predicted as decimal: 3624\n",
      "Decimal: 9447. Correct: False\n",
      "    Predicted Base-4: [2 1 1 2 2 3 3]\n",
      "    True Base-4     : [2 1 0 3 2 1 3]\n",
      "    Predicted as decimal: 9647\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Set device to cuda:1\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Parameters\n",
    "    input_digits = 10  # Number of decimal digits to encode\n",
    "    output_digits = 7  # Maximum number of base-4 digits\n",
    "    max_decimal = 4 ** output_digits - 1  # Maximum decimal number we can represent\n",
    "    hidden_size = 1024  # Size of hidden layer\n",
    "\n",
    "    # Generate data - adjust num_samples to be at most max_decimal+1\n",
    "    num_samples = min(20000, max_decimal + 1)\n",
    "    print(f\"Using {num_samples} samples (max possible: {max_decimal + 1})\")\n",
    "\n",
    "    X, y, decimal_nums = generate_training_data(\n",
    "        num_samples=num_samples,\n",
    "        max_decimal=max_decimal,\n",
    "        input_digits=input_digits,\n",
    "        output_digits=output_digits,\n",
    "    )\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    (\n",
    "        train_dataset,\n",
    "        test_dataset,\n",
    "        train_decimal_nums,\n",
    "        test_decimal_nums,\n",
    "    ) = train_test_split_data(X, y, decimal_nums, test_size=0.1)\n",
    "\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Testing samples: {len(test_dataset)}\")\n",
    "\n",
    "    # Create the model and move to device\n",
    "    model = DecimalToBase4NN(\n",
    "        input_digits=input_digits,\n",
    "        input_classes=10,  # 10 possible values (0-9) for decimal\n",
    "        hidden_size=hidden_size,\n",
    "        output_digits=output_digits,\n",
    "    ).to(device)\n",
    "\n",
    "    print(\"Initial model:\")\n",
    "    evaluate_model(model, test_dataset, device)\n",
    "    # Train the model\n",
    "    train_model(\n",
    "        model,\n",
    "        train_dataset,\n",
    "        device,\n",
    "        epochs=2000,  # Increased epochs\n",
    "        batch_size=512,\n",
    "        learning_rate=0.05,\n",
    "    )\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    digit_accuracy, number_accuracy = evaluate_model(model, test_dataset, device)\n",
    "\n",
    "    # Test specific examples\n",
    "    test_specific_examples(\n",
    "        model, test_decimal_nums, device, input_digits, output_digits, num_samples=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cbe141b4-e8ea-4ec6-beb4-fc9980c529d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing specific examples from the test set:\n",
      "Decimal: 12100. Correct: True\n",
      "    Predicted Base-4: [2 3 3 1 0 1 0]\n",
      "    True Base-4     : [2 3 3 1 0 1 0]\n",
      "Decimal: 11182. Correct: False\n",
      "    Predicted Base-4: [2 3 3 2 2 3 0]\n",
      "    True Base-4     : [2 2 3 2 2 3 2]\n",
      "    Predicted as decimal: 12204\n",
      "Decimal: 6990. Correct: False\n",
      "    Predicted Base-4: [1 2 2 1 0 3 2]\n",
      "    True Base-4     : [1 2 3 1 0 3 2]\n",
      "    Predicted as decimal: 6734\n",
      "Decimal: 3037. Correct: False\n",
      "    Predicted Base-4: [0 3 3 3 1 3 3]\n",
      "    True Base-4     : [0 2 3 3 1 3 1]\n",
      "    Predicted as decimal: 4063\n",
      "Decimal: 7449. Correct: True\n",
      "    Predicted Base-4: [1 3 1 0 1 2 1]\n",
      "    True Base-4     : [1 3 1 0 1 2 1]\n",
      "Decimal: 8475. Correct: False\n",
      "    Predicted Base-4: [2 0 1 2 1 1 3]\n",
      "    True Base-4     : [2 0 1 0 1 2 3]\n",
      "    Predicted as decimal: 8599\n",
      "Decimal: 14227. Correct: False\n",
      "    Predicted Base-4: [3 2 3 2 1 1 3]\n",
      "    True Base-4     : [3 1 3 2 1 0 3]\n",
      "    Predicted as decimal: 15255\n",
      "Decimal: 2737. Correct: False\n",
      "    Predicted Base-4: [0 2 2 2 3 0 3]\n",
      "    True Base-4     : [0 2 2 2 3 0 1]\n",
      "    Predicted as decimal: 2739\n",
      "Decimal: 15931. Correct: True\n",
      "    Predicted Base-4: [3 3 2 0 3 2 3]\n",
      "    True Base-4     : [3 3 2 0 3 2 3]\n",
      "Decimal: 4848. Correct: False\n",
      "    Predicted Base-4: [1 0 2 3 3 2 0]\n",
      "    True Base-4     : [1 0 2 3 3 0 0]\n",
      "    Predicted as decimal: 4856\n"
     ]
    }
   ],
   "source": [
    "test_specific_examples(\n",
    "    model, test_decimal_nums, device, input_digits, output_digits, num_samples=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a254c4f3-104c-4c77-8167-e714cfba907a",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a98f9c5-0d0f-4790-a157-7573df6cdffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simplegrad as sg\n",
    "import simplegrad.module as snn\n",
    "from simplegrad import Tensor, ops\n",
    "from simplegrad.module import Module\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3eda381-4eda-4e1d-8730-2caca5d3a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter(Tensor):\n",
    "    \"\"\"A special kind of tensor that represents parameters.\"\"\"\n",
    "\n",
    "\n",
    "def _unpack_params(value: object):\n",
    "    if isinstance(value, Parameter):\n",
    "        return [value]\n",
    "    elif isinstance(value, Module):\n",
    "        return value.parameters()\n",
    "    elif isinstance(value, dict):\n",
    "        params = []\n",
    "        for k, v in value.items():\n",
    "            params += _unpack_params(v)\n",
    "        return params\n",
    "    elif isinstance(value, (list, tuple)):\n",
    "        params = []\n",
    "        for v in value:\n",
    "            params += _unpack_params(v)\n",
    "        return params\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "class Module:\n",
    "    def parameters(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "\n",
    "    def parameters(self):  # List[Tensor]\n",
    "        \"\"\"Return the list of parameters in the module.\"\"\"\n",
    "        return _unpack_params(self.__dict__)\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        self.weight = Parameter(\n",
    "            np.random.randn(out_features, in_features) * 0.01, requires_grad=True\n",
    "        )\n",
    "        self.bias = Parameter(np.zeros(out_features), requires_grad=True)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        output = ops.matmul(x, self.weight.transpose())\n",
    "        output = output + ops.broadcast_to(self.bias, output.shape)\n",
    "        return output\n",
    "        # return matmul(x, self.weight.transpose()) + self.bias\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.weight.data = state_dict[\"weight\"].detach().numpy()\n",
    "        self.bias.data = state_dict[\"bias\"].detach().numpy()\n",
    "\n",
    "    def state_dict(self):\n",
    "        return dict(\n",
    "            weight=self.weight.data,\n",
    "            bias=self.bias.data,\n",
    "        )\n",
    "\n",
    "\n",
    "class ReLU(Module):\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        return ops.relu(x)\n",
    "\n",
    "\n",
    "class Sequential(Module):\n",
    "    def __init__(self, *modules):\n",
    "        super().__init__()\n",
    "        self.modules = modules\n",
    "\n",
    "    def __call__(self, x: Tensor) -> Tensor:\n",
    "        output = x\n",
    "        for module in self.modules:\n",
    "            output = module(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66158058-0569-44d1-8cc1-3ea3ab84dcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.randint(low=0, high=4, size=6)\n",
    "y = Tensor(y)\n",
    "label = one_hot(n_dim=4, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "917cacfa-8b42-4763-b73a-5304a2df7b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8388cd42-54bd-4395-9e6f-9d5e06716f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(n_dim: int, y: Tensor) -> Tensor:\n",
    "    \"\"\"\n",
    "    n_dim (int): Number of classes (length of each one-hot vector).\n",
    "    y (Tensor): Tensor of shape (batch_size,) with integer class indices.\n",
    "    \"\"\"\n",
    "    batch_size = y.shape[0]\n",
    "    one_hot_np = np.zeros((batch_size, n_dim), dtype=int)\n",
    "    for i in range(batch_size):\n",
    "        one_hot_np.data[i, int(y.data[i])] = 1\n",
    "    one_hot_tensor = Tensor(one_hot_np, requires_grad=False)\n",
    "    return one_hot_tensor\n",
    "\n",
    "class SoftmaxLoss(Module):\n",
    "    def __call__(self, logits: Tensor, y: Tensor):\n",
    "        n_dim = logits.shape[-1]\n",
    "        y_one_hot = one_hot(n_dim, y)\n",
    "        zy = ops.summation(logits * y_one_hot, axes=(1,))\n",
    "        zy = ops.reshape(zy, (-1, 1))\n",
    "        zy = ops.broadcast_to(zy, logits.shape)\n",
    "\n",
    "        losses = ops.logsumexp(logits - zy, axes=(1))\n",
    "        total_loss = ops.summation(losses) / losses.shape[-1]\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e545a0d-9339-433a-9ed3-4fe244b1a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def reset_grad(self):\n",
    "        for p in self.params:\n",
    "            p.grad = None\n",
    "\n",
    "\n",
    "class NaiveSGD(Optimizer):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent (SGD) with optional L2 regularization.\n",
    "\n",
    "    - SGD minimizes a loss by updating parameters opposite to the gradient.\n",
    "    - L2-regularized loss: \\( L_{\\text{reg}}(w) = L(w) + \\frac{\\lambda}{2} \\|w\\|_2^2 \\)\n",
    "      - \\( \\|w\\|_2^2 \\): squared L2 norm of parameters.\n",
    "      - \\( \\lambda \\): regularization strength (`weight_decay`).\n",
    "    - Gradient: \\( \\nabla L_{\\text{reg}}(w) = \\nabla L(w) + \\lambda w \\)\n",
    "    - Update rule: \\( w \\leftarrow w - \\eta (\\nabla L(w) + \\lambda w) \\)\n",
    "      - \\( \\eta \\): learning rate (`lr`).\n",
    "    - Implementation: adds \\( \\lambda w \\) to gradient as weight decay.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=0.01, weight_decay=0.0):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def step(self):\n",
    "        for w in self.params:\n",
    "            gradient = w.grad + self.weight_decay * w.data\n",
    "            w.data = w.data - self.lr * gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead8ea6d-8494-429c-85ec-696db0742c02",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test forward, backward, step for linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "f0303af9-02be-4c40-8401-7fe709762c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def test_linear_vs_pytorch():\n",
    "    # Test parameters\n",
    "    in_features, out_features = 20, 30\n",
    "    batch_size = 2\n",
    "    lr = 0.005\n",
    "\n",
    "    # Pre-generate weights for identical initialization\n",
    "    weight = np.random.randn(out_features, in_features).astype(np.float32) * 0.01\n",
    "    bias = np.zeros(out_features, dtype=np.float32)\n",
    "\n",
    "    # Initialize your implementation with fixed weights\n",
    "    linear = Linear(in_features, out_features)\n",
    "    linear.weight.data = weight.copy()\n",
    "    linear.bias.data = bias.copy()\n",
    "\n",
    "    # Initialize PyTorch equivalent with same weights\n",
    "    torch_linear = nn.Linear(in_features, out_features, bias=True)\n",
    "    torch_linear.weight.data = torch.tensor(weight, dtype=torch.float32)\n",
    "    torch_linear.bias.data = torch.tensor(bias, dtype=torch.float32)\n",
    "\n",
    "    # Generate random input\n",
    "    np_x = np.random.randn(batch_size, in_features).astype(np.float32)\n",
    "    x = Tensor(np_x, requires_grad=False)\n",
    "    torch_x = torch.tensor(np_x, requires_grad=False, dtype=torch.float32)\n",
    "\n",
    "    # Random target\n",
    "    np_y = np.random.randn(batch_size, out_features).astype(np.float32)\n",
    "    y = Tensor(np_y, requires_grad=False)\n",
    "    torch_y = torch.tensor(np_y, requires_grad=False, dtype=torch.float32)\n",
    "\n",
    "    # Forward pass\n",
    "    output = linear(x)\n",
    "    torch_output = torch_linear(torch_x)\n",
    "    # return output, torch_output\n",
    "    # Check output\n",
    "    np.testing.assert_allclose(output.data, torch_output.detach().numpy(), rtol=1e-4)\n",
    "    print(\"Forward pass matches PyTorch ✓\")\n",
    "\n",
    "    # Loss\n",
    "    loss = summation((output - y) ** 2)\n",
    "    torch_loss = ((torch_output - torch_y) ** 2).sum()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    torch_loss.backward()\n",
    "\n",
    "    # Check gradients\n",
    "    np.testing.assert_allclose(\n",
    "        linear.weight.grad, torch_linear.weight.grad.numpy(), rtol=1e-4\n",
    "    )\n",
    "    np.testing.assert_allclose(\n",
    "        linear.bias.grad, torch_linear.bias.grad.numpy(), rtol=1e-4\n",
    "    )\n",
    "    print(\"Gradients match PyTorch ✓\")\n",
    "\n",
    "    # Optimizer step\n",
    "    opt = NaiveSGD([linear.weight, linear.bias], lr=lr)\n",
    "    torch_opt = torch.optim.SGD(torch_linear.parameters(), lr=lr)\n",
    "\n",
    "    opt.step()\n",
    "    torch_opt.step()\n",
    "\n",
    "    # Check updated weights\n",
    "    np.testing.assert_allclose(\n",
    "        linear.weight.data, torch_linear.weight.data.numpy(), rtol=1e-4\n",
    "    )\n",
    "    np.testing.assert_allclose(\n",
    "        linear.bias.data, torch_linear.bias.data.numpy(), rtol=1e-4\n",
    "    )\n",
    "    print(\"Parameter updates match PyTorch ✓\")\n",
    "\n",
    "    print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6a13d8-367e-4442-9904-8cc7a525198a",
   "metadata": {},
   "source": [
    "#### Continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cfb907a-ca9f-4883-886b-98734e2aa697",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = snn.Sequential(snn.Linear(3, 4), snn.ReLU(), snn.Linear(4, 3))\n",
    "sgd = NaiveSGD(net.parameters(), lr=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a2352a-c84f-4f1c-860b-a6bd5935c5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040e391b-e0e2-4788-9e43-0b8cd94edf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    device,\n",
    "    epochs=100,\n",
    "    batch_size=128,\n",
    "    learning_rate=0.01,\n",
    "    log_steps=50,\n",
    "):\n",
    "    \"\"\"Train the neural network using SGD optimizer.\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "    # Create DataLoader for training data\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # Move batch to device\n",
    "            batch_X = batch_X.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(batch_X)\n",
    "\n",
    "            # Reshape for cross entropy loss\n",
    "            batch_size = batch_X.size(0)\n",
    "            predictions = predictions.view(batch_size * model.output_digits, 4)\n",
    "            targets = batch_y.view(batch_size * model.output_digits, 4)\n",
    "            targets = torch.argmax(targets, dim=1)  # Convert one-hot to class indices\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(predictions, targets)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        if (epoch + 1) % log_steps == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6a8a50-a382-44a4-90e6-e445331a4e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "merger2",
   "language": "python",
   "name": "merger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
